# -*- coding: utf-8 -*-
"""notebook_fixed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DhMpp553nC9dBCyDpKTmE_wGASspw7rb
"""

import os
from pathlib import Path

from google.colab import drive
drive.mount('/content/drive')

root_path = Path("/content/drive/MyDrive/6.BTC_Venues")

folders = [
    "data/raw", "data/processed",
    "notebooks",
    "src/data_loader", "src/analysis", "src/utils",
    "config"
]

files = [
    "src/__init__.py", "src/data_loader/__init__.py", "src/data_loader/base.py",
    "src/data_loader/binance.py", "src/data_loader/gateio.py",
    "src/analysis/__init__.py", "src/analysis/lead_lag.py", "src/analysis/flow_toxicity.py", "src/analysis/microstructure.py",
    "src/utils/__init__.py", "src/utils/time_alignment.py", "src/utils/numba_ops.py",
    "config/settings.yaml", "main.py", "requirements.txt", "README.md", ".gitignore"
]

for folder in folders:
    (root_path / folder).mkdir(parents=True, exist_ok=True)

for file in files:
    f_path = root_path / file
    if not f_path.exists():
        f_path.touch()

print(f" Structure initialized at: {root_path}")

import json
import os

def strip_widgets_from_notebooks(root_dir="."):
    """
    æš´åŠ›åˆ é™¤ repo ä¸‹æ‰€æœ‰ notebook çš„ metadata.widgets
    """
    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".ipynb"):
                full_path = os.path.join(subdir, file)
                try:
                    with open(full_path, "r", encoding="utf-8") as f:
                        nb = json.load(f)

                    changed = False
                    for cell in nb.get("cells", []):
                        if "widgets" in cell.get("metadata", {}):
                            cell["metadata"].pop("widgets")
                            changed = True

                    if changed:
                        with open(full_path, "w", encoding="utf-8") as f:
                            json.dump(nb, f, indent=1)
                        print(f"ğŸ’¾ Fixed widgets: {full_path}")
                    else:
                        print(f"âœ… No widgets to fix: {full_path}")
                except Exception as e:
                    print(f"âŒ Failed: {full_path}, {e}")

# æ‰§è¡Œ
strip_widgets_from_notebooks(".")  # "." è¡¨ç¤ºå½“å‰ç›®å½•ï¼Œå¯æ”¹æˆä½ çš„ repo è·¯å¾„

from google.colab import drive
import os

print("ğŸ”Œ Mounting Google Drive...")
drive.mount('/content/drive')

# æ£€æŸ¥ä¸€ä¸‹è·¯å¾„æ˜¯å¦çœŸçš„å­˜åœ¨ï¼Œé˜²æ­¢æ‹¼å†™é”™è¯¯
target_path = "/content/drive/MyDrive/6.BTC_Venues/data/raw"
if not os.path.exists(target_path):
    print(f"âš ï¸ Warning: Path {target_path} does not exist yet. Creating it now...")
    os.makedirs(target_path, exist_ok=True)
else:
    print(f"âœ… Path found: {target_path}")







"""# DATA"""





"""## 2ï¸âƒ£ src/data_loader/binance.py (å¸å®‰è§£æå™¨)"""

import os
import zipfile
import polars as pl
from pathlib import Path
from tqdm.notebook import tqdm  # å¦‚æœä½ åœ¨è„šæœ¬è¿è¡Œï¼Œæ”¹æˆ from tqdm import tqdm
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import io

# Setup logging - å› ä¸ºä½œä¸º ENTJï¼Œä½ éœ€è¦æŒæ§ä¸€åˆ‡ç»†èŠ‚
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BinanceExtremeDownloader:
    def __init__(self, symbol: str, start_date: str, end_date: str, base_dir="/content/drive/MyDrive/6.BTC_Venues/data"):
        """
        Args:
            symbol (str): e.g., "BTCUSDT"
            start_date (str): "YYYY-MM-DD"
            end_date (str): "YYYY-MM-DD"
        """
        self.symbol = symbol.upper()
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
        self.end_date = datetime.strptime(end_date, "%Y-%m-%d")

        # 1. Base URL for Futures Daily Trades
        # Check validity: https://data.binance.vision/?prefix=data/futures/um/daily/trades/BTCUSDT/
        self.base_url = "https://data.binance.vision/data/futures/um/daily/trades"

        # Paths
        self.base_dir = Path(base_dir)
        self.raw_dir = self.base_dir / "raw" / "temp_daily" / self.symbol
        self.processed_dir = self.base_dir / "processed" / self.symbol  # è¡¥å…¨äº†ä½ çš„æ‚¬å¿µ

        # Ensure directories exist
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)

    def generate_date_range(self):
        """Generates a list of dates between start and end."""
        delta = self.end_date - self.start_date
        return [self.start_date + timedelta(days=i) for i in range(delta.days + 1)]

    def download_and_extract(self, date_obj):
        """
        Downloads the ZIP, extracts CSV in memory, converts to Polars DataFrame, saves as Parquet.
        Efficiency is key, Dr. Gao.
        """
        date_str = date_obj.strftime("%Y-%m-%d")
        file_name = f"{self.symbol}-trades-{date_str}"
        zip_name = f"{file_name}.zip"
        url = f"{self.base_url}/{self.symbol}/{zip_name}"

        save_path = self.processed_dir / f"{file_name}.parquet"

        # Skip if already exists (Idempotency is a virtue)
        if save_path.exists():
            return f"Skipped {date_str} (Already exists)"

        try:
            with requests.get(url, stream=True, timeout=10) as r:
                if r.status_code == 404:
                    return f"Warning: Data not found for {date_str} (404)"
                r.raise_for_status()

                # In-memory extraction to save disk I/O cycles
                z = zipfile.ZipFile(io.BytesIO(r.content))
                csv_name = z.namelist()[0]

                # Read CSV directly into Polars from bytes
                # Binance trade columns: id, price, qty, quote_qty, time, is_buyer_maker
                with z.open(csv_name) as f:
                    df = pl.read_csv(
                        f.read(),
                        has_header=False,
                        new_columns=["id", "price", "qty", "quote_qty", "time", "is_buyer_maker"],
                        schema_overrides={"id": pl.Int64, "price": pl.Float64, "qty": pl.Float64, "quote_qty": pl.Float64, "time": pl.Int64, "is_buyer_maker": pl.Boolean}
                    )

                # Convert timestamp to datetime immediately
                df = df.with_columns(
                    pl.from_epoch("time", time_unit="ms").alias("datetime")
                )

                # Write to Parquet (Smaller, Faster, Better)
                df.write_parquet(save_path, compression="snappy")

                return f"Success: {date_str} -> {len(df)} rows"

        except Exception as e:
            return f"Error downloading {date_str}: {str(e)}"

    def run(self, max_workers=8):
        """
        Orchestrate the download with multi-threading.
        """
        dates = self.generate_date_range()
        logger.info(f"Starting download for {self.symbol} from {self.start_date.date()} to {self.end_date.date()}")
        logger.info(f"Target Directory: {self.processed_dir}")

        results = []
        # TQDM progress bar for that dopamine hit when it reaches 100%
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_date = {executor.submit(self.download_and_extract, d): d for d in dates}

            for future in tqdm(as_completed(future_to_date), total=len(dates), desc="Downloading & Processing"):
                res = future.result()
                results.append(res)
                # Optional: Log errors immediately
                if "Error" in res or "Warning" in res:
                    logger.warning(res)

        logger.info("Mission Complete. Data is ready for your Alpha extraction.")

# --- Execution Block ---
# Dr. Gao, define your parameters here. Don't be shy.
if __name__ == "__main__":
    # Example usage:
    downloader = BinanceExtremeDownloader(
        symbol="BTCUSDT",
        start_date="2024-01-01",
        end_date="2024-01-05", # Just a sample range
        base_dir="." # Changed to current dir for testing, change back to your Drive path
    )

    downloader.run(max_workers=16) # 16 threads because we are aggressive.

import os
import zipfile
import polars as pl
from pathlib import Path
from tqdm.notebook import tqdm  # å¦‚æœåœ¨éNotebookç¯å¢ƒï¼Œè¯·ç”¨ from tqdm import tqdm
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import io
import random

# --- 1. Logging Setup (ENTJ Style: Clear & Direct) ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("BinanceQuant")

# --- 2. The Core Class ---
class BinanceExtremeDownloader:
    def __init__(self, symbol: str, start_date: str, end_date: str, base_dir="/content/drive/MyDrive/6.BTC_Venues/data"):
        self.symbol = symbol.upper()
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
        self.end_date = datetime.strptime(end_date, "%Y-%m-%d")

        # Binance Data Vision URL
        self.base_url = "https://data.binance.vision/data/futures/um/daily/trades"

        # Directories
        self.base_dir = Path(base_dir)
        self.processed_dir = self.base_dir / "processed" / self.symbol

        # Create directories immediately
        self.processed_dir.mkdir(parents=True, exist_ok=True)

    def generate_date_range(self):
        """Yields the timeline of our conquest."""
        delta = self.end_date - self.start_date
        return [self.start_date + timedelta(days=i) for i in range(delta.days + 1)]

    def download_and_extract(self, date_obj):
        """
        The heavy lifter. Downloads ZIP -> Memory -> Header Check -> Polars -> Parquet.
        """
        date_str = date_obj.strftime("%Y-%m-%d")
        file_name = f"{self.symbol}-trades-{date_str}"
        zip_name = f"{file_name}.zip"
        url = f"{self.base_url}/{self.symbol}/{zip_name}"

        save_path = self.processed_dir / f"{file_name}.parquet"

        # Idempotency Check
        if save_path.exists():
            return f"Skipped: {date_str} (Exists)"

        try:
            with requests.get(url, stream=True, timeout=15) as r:
                if r.status_code == 404:
                    return f"Missing: {date_str} (404 on Server)"
                r.raise_for_status()

                # In-memory Unzipping
                z = zipfile.ZipFile(io.BytesIO(r.content))
                csv_name = z.namelist()[0]

                with z.open(csv_name) as f:
                    # --- INTELLIGENT HEADER DETECTION ---
                    # Peek at the first line to decide strategy
                    first_line = f.readline().decode('utf-8')
                    # If 'id' and 'price' are in the first line, it's a header.
                    has_header = "id" in first_line.lower() and "price" in first_line.lower()

                    # CRITICAL: Reset pointer to 0, or we lose the first row
                    f.seek(0)
                    content = f.read()

                    # Polars parsing with strict types
                    df = pl.read_csv(
                        content,
                        has_header=has_header,
                        new_columns=["id", "price", "qty", "quote_qty", "time", "is_buyer_maker"],
                        schema_overrides={
                            "id": pl.Int64,
                            "price": pl.Float64,
                            "qty": pl.Float64,
                            "quote_qty": pl.Float64,
                            "time": pl.Int64,
                            "is_buyer_maker": pl.Boolean
                        }
                    )

                if df.height == 0:
                    return f"Empty: {date_str} has 0 rows"

                # Standardize Timestamp
                df = df.with_columns(
                    pl.from_epoch("time", time_unit="ms").alias("datetime")
                )

                # Save to Parquet (Snappy Compressed)
                df.write_parquet(save_path, compression="snappy")

                return f"Success: {date_str} -> {len(df):,} rows (Header: {has_header})"

        except Exception as e:
            return f"Error: {date_str} -> {str(e)}"

    def run(self, max_workers=8):
        """
        Execute the download plan.
        """
        dates = self.generate_date_range()
        logger.info(f"--- Mission Start: {self.symbol} ---")
        logger.info(f"Range: {self.start_date.date()} to {self.end_date.date()}")
        logger.info(f"Storage: {self.processed_dir}")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Map futures
            future_to_date = {executor.submit(self.download_and_extract, d): d for d in dates}

            # Progress Bar
            for future in tqdm(as_completed(future_to_date), total=len(dates), desc="Processing"):
                res = future.result()
                # Log errors or warnings, keep clean success silent-ish
                if "Error" in res or "Missing" in res or "Warning" in res:
                    logger.warning(res)
                # else:
                    # logger.info(res) # Uncomment for verbose mode

        logger.info("--- Mission Complete ---")

    def preview_data(self):
        """
        Randomly inspect a saved file to verify integrity.
        Dr. Gao needs empirical evidence.
        """
        files = list(self.processed_dir.glob("*.parquet"))
        if not files:
            logger.warning("No files found to preview.")
            return

        target = random.choice(files)
        logger.info(f"Inspecting random file: {target.name}")

        try:
            df = pl.read_parquet(target)
            print("\n--- Data Schema ---")
            print(df.schema)
            print("\n--- First 5 Rows ---")
            print(df.head(5))
            print("\n--- Stats ---")
            print(df.describe())
        except Exception as e:
            logger.error(f"Failed to read file: {e}")

# --- 3. Execution Block ---
if __name__ == "__main__":
    # Dr. Gao, set your target here:
    downloader = BinanceExtremeDownloader(
        symbol="BTCUSDT",
        start_date="2026-01-01",  # ä½¿ç”¨ä½ ä¹‹å‰æŠ¥é”™çš„æ—¶é—´æ®µè¿›è¡Œæµ‹è¯•
        end_date="2026-01-31",
        base_dir="/content/drive/MyDrive/6.BTC_Venues/data"           # è¿™é‡Œæ”¹ä¸º "." æ–¹ä¾¿å½“å‰ç›®å½•æµ‹è¯•ï¼Œç”Ÿäº§ç¯å¢ƒè¯·æ”¹å› Drive è·¯å¾„
    )

    # 1. Run Download
    downloader.run(max_workers=8)

    # 2. Verify Result
    print("\n" + "="*30)
    print("ğŸ‘€ Dr. Gao's Inspection Time")
    print("="*30)
    downloader.preview_data()

import polars as pl
from pathlib import Path
import logging
import time

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("BigDataMerger")

def merge_massive_data(symbol: str, target_month: str, base_dir: str):
    """
    Merges daily files into a monthly monster using Streaming API.
    Prevents OOM (Out of Memory) even on modest machines.
    """
    base_path = Path(base_dir)
    source_dir = base_path / "processed" / symbol

    # ç›®æ ‡æ–‡ä»¶å
    output_filename = f"{symbol}-{target_month}-FULL.parquet"
    output_path = base_path / "processed" / output_filename

    # 1. Glob Pattern - æ‰«ææ‰€æœ‰ç›¸å…³æ–‡ä»¶
    file_pattern = str(source_dir / f"{symbol}-trades-*.parquet")
    logger.info(f"Target Source: {file_pattern}")

    try:
        start_time = time.time()

        # 2. Lazy Scan (ä¸è¯»å–æ•°æ®ï¼Œåªè¯»å– Metadata)
        # è¿™å°±æ˜¯è¿ç­¹å¸·å¹„ï¼Œå†³èƒœåƒé‡Œã€‚
        q = pl.scan_parquet(file_pattern)

        # 3. Sort is Mandatory
        # æ•£ä¹±çš„æ—¶é—´åºåˆ—æ²¡æœ‰ä»·å€¼ã€‚æˆ‘ä»¬å¿…é¡»æŒ‰æ—¶é—´é‡æ’ã€‚
        # å³ä½¿æ–‡ä»¶æ˜¯åˆ†å¼€çš„ï¼Œåˆå¹¶åå¿…é¡»ä¿è¯å…¨å±€æœ‰åºã€‚
        q = q.sort("time")

        # 4. Sink to Parquet (The Magic)
        # è¿™é‡Œçš„ sink_parquet ä¼šå¯åŠ¨ Streaming å¼•æ“ï¼Œåˆ†å—å¤„ç†å¹¶å†™å…¥ç¡¬ç›˜ã€‚
        # å†…å­˜å ç”¨æä½ï¼Œå“ªæ€•ä½ æœ‰ 100 äº¿è¡Œä¹Ÿèƒ½è·‘ã€‚
        logger.info(f"Stream-merging to: {output_path} ... This may take a minute.")
        q.sink_parquet(output_path, compression="snappy")

        elapsed = time.time() - start_time
        logger.info(f"âœ… Merge Complete in {elapsed:.2f} seconds.")

        return output_path

    except Exception as e:
        logger.error(f"Merge Failed: {e}")
        return None

def inspect_monster(file_path):
    """
    Quick health check on the massive file without loading it all.
    """
    if not file_path or not file_path.exists():
        return

    print("\n" + "="*40)
    print(f"ğŸ¦– MONSTER FILE INSPECTION: {file_path.name}")
    print("="*40)

    # è·å–æ–‡ä»¶å¤§å°
    file_size_mb = file_path.stat().st_size / (1024 * 1024)
    print(f"File Size   : {file_size_mb:.2f} MB")

    # Scan again to count rows (Very fast with Parquet metadata)
    lf = pl.scan_parquet(file_path)
    row_count = lf.select(pl.len()).collect().item()

    print(f"Total Rows  : {row_count:,}")

    # Peek at the time range
    time_stats = lf.select([
        pl.col("datetime").min().alias("start"),
        pl.col("datetime").max().alias("end")
    ]).collect()

    print(f"Time Start  : {time_stats['start'][0]}")
    print(f"Time End    : {time_stats['end'][0]}")
    print("="*40)

# --- Execution ---
if __name__ == "__main__":
    # Dr. Gao's Config
    base_dir = "/content/drive/MyDrive/6.BTC_Venues/data"

    # 1. Merge
    final_file = merge_massive_data(
        symbol="BTCUSDT",
        target_month="2026-01",
        base_dir=base_dir
    )

    # 2. Inspect
    inspect_monster(final_file)

import polars as pl
import matplotlib.pyplot as plt
from pathlib import Path

# --- 1. å®šä½æ•°æ® (Target Lock) ---
base_dir = Path("/content/drive/MyDrive/6.BTC_Venues/data/processed")
# ä¼˜å…ˆæ‰¾é‚£ä¸ªå·¨å¤§çš„åˆå¹¶æ–‡ä»¶ï¼Œæ‰¾ä¸åˆ°å°±éšä¾¿æ‰¾ä¸ª daily æ–‡ä»¶åšæµ‹è¯•
merged_file = base_dir / "BTCUSDT-2026-01-FULL.parquet"

# è¿™é‡Œç”¨ scan_parquet è€Œä¸æ˜¯ read_parquetï¼
# scan = Lazy (ä¸å å†…å­˜), read = Eager (åƒå…‰å†…å­˜)
if merged_file.exists():
    print(f"ğŸ¯ Found Monster File: {merged_file.name}")
    q = pl.scan_parquet(merged_file)
else:
    print("âš ï¸ Full file not found. Scanning for daily chunks...")
    chunk_dir = base_dir / "BTCUSDT" # æ³¨æ„è·¯å¾„å¯èƒ½æ ¹æ®ä¹‹å‰ä»£ç æœ‰æ‰€ä¸åŒ
    files = sorted(list(chunk_dir.glob("*.parquet")))
    if not files:
        raise FileNotFoundError("ğŸš¨ No data found! Please run the downloader first.")

    print(f"ğŸ‘€ Detected {len(files)} chunks. Lazily scanning them all...")
    q = pl.scan_parquet(files) # Polars å¯ä»¥ç›´æ¥æ‰«ä¸€å †æ–‡ä»¶ï¼Œç‰›ä¸ç‰›ï¼Ÿ

# --- 2. å±•ç¤ºå‰ 10 è¡Œ (The Appetizer) ---
print("\n" + "="*50)
print("ğŸ§ FIRST 10 ROWS (Zero RAM Impact)")
print("="*50)
# åªè¯»å–å‰ 10 è¡Œï¼Œå†…å­˜æ¶ˆè€—å‡ ä¹ä¸º 0
print(q.head(10).collect())

# --- 3. æ ¸å¿ƒæŒ‡æ ‡ä½“æ£€ (The High-IQ Diagnosis) ---
print("\n" + "="*50)
print("ğŸ©º DATA HEALTH CHECK REPORT (Streaming Computation)")
print("="*50)

# è¿™é‡Œæˆ‘ä»¬è¦è®¡ç®—å…¨é‡ç»Ÿè®¡ï¼Œä½† Polars ä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼Œåˆ†å—æµå¼å¤„ç†ï¼Œä¸ä¼šå´© RAMã€‚
# æ³¨æ„ï¼šä¹‹å‰çš„ä»£ç é‡Œæˆ‘ä»¬æ²¡æœ‰ 'side' åˆ—ï¼Œè€Œæ˜¯ 'is_buyer_maker'ã€‚
# is_buyer_maker = True  -> Maker is Buyer (Taker is Seller) -> Active Sell
# is_buyer_maker = False -> Maker is Seller (Taker is Buyer) -> Active Buy

stats = q.select([
    pl.len().alias("total_rows"),
    pl.col("datetime").min().alias("start_time"),
    pl.col("datetime").max().alias("end_time"),
    pl.col("price").min().alias("min_price"),
    pl.col("price").max().alias("max_price"),
    pl.col("price").null_count().alias("null_prices"),
    # è®¡ç®—ä¸»åŠ¨ä¹°å…¥æ¯”ä¾‹: is_buyer_maker ä¸º False çš„æ¯”ä¾‹
    (1 - pl.col("is_buyer_maker").mean()).alias("active_buy_ratio")
]).collect()

# è§£åŒ…æ•°æ®
row = stats.row(0)
total_rows = row[0]
start_time = row[1]
end_time = row[2]
min_price = row[3]
max_price = row[4]
null_prices = row[5]
buy_ratio = row[6]

print(f"Total Ticks    : {total_rows:,}")
print(f"Time Range     : {start_time} <---> {end_time}")
print(f"Price Range    : {min_price} - {max_price}")

# Logic Checks
if null_prices > 0:
    print(f"ğŸš¨ WARNING: {null_prices} null prices detected!")
else:
    print("âœ… Integrity Check : Pass (No Nulls)")

if min_price <= 0:
    print("ğŸš¨ CRITICAL: Zero or Negative prices detected!")
else:
    print("âœ… Price Logic     : Pass (> 0)")

print(f"Active Buy Ratio : {buy_ratio:.2%}")
if buy_ratio > 0.5:
    print("   -> Sentiment    : Bullish (Buyers are aggressive)")
else:
    print("   -> Sentiment    : Bearish (Sellers are aggressive)")

# --- 4. å¿«é€Ÿå¯è§†åŒ– (Visual Confirmation) ---
# åƒä¸‡ä¸è¦ plt.plot(df) å…¨é‡ç”»å›¾ï¼Œé‚£æ˜¯è‡ªæ€ã€‚
# æˆ‘ä»¬åªå–å‡ºå‰ 1000 ä¸ªç‚¹åˆ°å†…å­˜é‡Œç”»å›¾ã€‚
print("\nğŸ¨ Plotting first 1000 ticks price path...")

# åªå–å‰ 1000 è¡Œ -> è½¬ Pandas (å› ä¸º Matplotlib éœ€è¦)
# è¿™æ—¶å€™è½¬ Pandas æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºåªæœ‰ 1000 è¡Œ
subset = q.select(["datetime", "price"]).head(1000).collect().to_pandas()

plt.figure(figsize=(12, 4))
plt.plot(subset['datetime'], subset['price'], label='Execution Price', linewidth=1, color='#F7931A') # BTC Color
plt.title(f"Microstructure Price Path (First 1000 Ticks) - {symbol if 'symbol' in locals() else 'BTCUSDT'}")
plt.ylabel("Price (USDT)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

import polars as pl
import os
import gc

def refine_binance_data(input_path, output_path):
    print(f"ğŸ”ª [Dr. Gao's Lab] æ­£åœ¨å¯¹ Binance æ•°æ®æ‰§è¡Œç˜¦èº«æ‰‹æœ¯...")

    # ä½¿ç”¨ Lazy æ¨¡å¼ï¼Œåªè¯»ä¸å å†…å­˜
    lf = pl.scan_parquet(input_path)

    lf_refined = (
        lf.select([
            pl.col("id"),
            pl.col("price").cast(pl.Float64),
            pl.col("qty").cast(pl.Float64),
            # è¿™é‡Œçš„é€»è¾‘ï¼šBuyer Maker = True ä»£è¡¨å–å•(Taker Sell)ï¼Œæ‰€ä»¥èµ‹ -1
            pl.when(pl.col("is_buyer_maker"))
              .then(pl.lit(-1, dtype=pl.Int8))
              .otherwise(pl.lit(1, dtype=pl.Int8))
              .alias("side_code"),
            pl.col("time"),
            pl.col("datetime").cast(pl.Datetime("ns")) # ç»Ÿä¸€æå‡åˆ°çº³ç§’ç²¾åº¦ï¼Œé˜²æ­¢å¯¹é½å‡ºé”™
        ])
    )

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    print(f"ğŸ’¾ æ­£åœ¨å†™å…¥ç²¾ç®€ç‰ˆ Parquet...")
    lf_refined.sink_parquet(
        output_path,
        compression="zstd",
        row_group_size=100_000
    )
    print(f"âœ… æˆåŠŸï¼æ–‡ä»¶å·²å­˜è‡³: {output_path}")

# --- é…ç½® ---
binance_in = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTCUSDT-2026-01-FULL.parquet"
binance_out = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTCUSDT-2026-01-Binance.parquet"

if __name__ == "__main__":
    gc.collect()
    try:
        refine_binance_data(binance_in, binance_out)

        # éªŒè¯ä¸€ä¸‹ç»“æœ
        print("\nğŸ§ æŠ½æŸ¥å‰ 5 è¡Œ:")
        print(pl.read_parquet(binance_out, n_rows=5))
    except Exception as e:
        print(f"âŒ è¿˜æ˜¯å‡ºäº†ç‚¹å°å·®é”™: {e}")

"""## 3ï¸âƒ£ src/data_loader/gateio.py (Gateè§£æå™¨)"""



from google.colab import drive
drive.mount('/content/drive')



import pandas as pd
import os
import gzip

def robust_gate_processor(file_path, output_dir):
    """
    é«˜åšå£«ä¸“ç”¨ï¼šå…·å¤‡è‡ªé€‚åº”èƒ½åŠ›çš„ Trades æ•°æ®å¤„ç†å™¨
    """
    file_name = os.path.basename(file_path)
    base_name = file_name.replace('.csv.gz', '').replace('.csv', '')
    output_file = os.path.join(output_dir, f"{base_name}_fixed.parquet")

    print(f"ğŸ§ æ­£åœ¨å—…æ¢æ–‡ä»¶ç»“æ„: {file_name}")

    try:
        # 1. å—…æ¢å‰å‡ è¡Œï¼Œçœ‹çœ‹è¿™å¸®ç¨‹åºå‘˜åˆ°åº•å†™äº†ä»€ä¹ˆ Header
        with gzip.open(file_path, 'rt') as f:
            first_line = f.readline().strip().lower()
            print(f"ğŸ‘€ æ–‡ä»¶é¦–è¡Œå†…å®¹: {first_line}")

        # 2. é€»è¾‘åˆ¤å®šï¼šå¦‚æœæ²¡æœ‰ price è¿™ä¸ªè¯ï¼Œè¯´æ˜æ²¡æœ‰ Header æˆ–è€…åˆ—åä¸å¯¹
        if 'price' not in first_line:
            print("âš ï¸ æœªå‘ç°æ ‡å‡† Headerï¼Œå¯åŠ¨å¼ºåˆ¶åˆ—åæ³¨å…¥æ¨¡å¼...")
            # æ ¹æ® Gate.io å®˜æ–¹æ–‡æ¡£å’Œä½ çš„æˆªå›¾ï¼Œåˆ—é¡ºåºé€šå¸¸æ˜¯ï¼š
            # timestamp, deal_id, price, amount (size), side
            col_names = ['timestamp', 'deal_id', 'price', 'size', 'side']
            header_setting = None # å‘Šè¯‰ pandas åˆ«æŠŠç¬¬ä¸€è¡Œå½“åˆ—å
        else:
            print("âœ… å‘ç° Headerï¼Œæ­£åœ¨è¿›è¡Œæ¸…æ´—...")
            col_names = None
            header_setting = 0

        # 3. å¼€å§‹åˆ†å—å¤„ç†
        chunks = pd.read_csv(
            file_path,
            compression='gzip',
            names=col_names,
            header=header_setting,
            chunksize=500000,
            on_bad_lines='skip' # è·³è¿‡å¯èƒ½å­˜åœ¨çš„è„æ•°æ®
        )

        processed_chunks = []
        for i, chunk in enumerate(chunks):
            # ç»Ÿä¸€åˆ—åï¼ˆä»¥é˜²ä¸‡ä¸€æœ‰å¤§å†™ï¼‰
            chunk.columns = [c.lower() for c in chunk.columns]

            # å…³é”®å­—æ®µå¼ºåˆ¶è½¬æ¢
            chunk['price'] = pd.to_numeric(chunk['price'], errors='coerce')
            chunk['size'] = pd.to_numeric(chunk['size'], errors='coerce')
            chunk['timestamp'] = pd.to_numeric(chunk['timestamp'], errors='coerce')

            # å‰”é™¤æ— æ•ˆè¡Œ
            chunk = chunk.dropna(subset=['price', 'timestamp'])

            # æ—¶é—´æˆ³è½¬æ¢
            chunk['datetime'] = pd.to_datetime(chunk['timestamp'], unit='s')

            processed_chunks.append(chunk)
            if i % 5 == 0:
                print(f"   å·²åå™¬ { (i+1)*0.5 }M è¡Œæ•°æ®...")

        # 4. åˆå¹¶å¹¶å¯¼å‡º
        df_final = pd.concat(processed_chunks)
        df_final.to_parquet(output_file, index=False)

        print(f"ğŸ å¤„ç†åœ†æ»¡å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆæ ·æœ¬é‡: {len(df_final)} è¡Œ")
        return df_final.head()

    except Exception as e:
        print(f"ğŸ”¥ å½»åº•ç¿»è½¦: {str(e)}")
        import traceback
        traceback.print_exc()

# å†æ¬¡å°è¯•
input_file = "/content/drive/MyDrive/6.BTC_Venues/data/raw/BTC_USDT-202601.csv.gz"
output_path = "/content/drive/MyDrive/6.BTC_Venues/data/raw"
preview = robust_gate_processor(input_file, output_path)
print(preview)

import pandas as pd

# å‡è®¾ df æ˜¯ä½ å·²ç»åŠ è½½å¥½çš„ dataframe
# å¦‚æœæ˜¯ä»æ–‡ä»¶è¯»å–ï¼šdf = pd.read_parquet("/content/drive/MyDrive/6.BTC_Venues/data/raw/BTC_USDT-202601_fixed.parquet")
df = pd.read_parquet("/content/drive/MyDrive/6.BTC_Venues/data/raw/BTC_USDT-202601_fixed.parquet")

def fix_and_inspect(df):
    # 1. ä¿®å¤ Side é€»è¾‘ (ä½¿ç”¨æ›´é«˜æ•ˆçš„ np.where è€Œä¸æ˜¯ apply)
    import numpy as np
    df['side'] = np.where(df['size'] > 0, 'buy', 'sell')

    # 2. å¢åŠ ä¸€ä¸ª 'amount' åˆ—ï¼ˆç»å¯¹å€¼æˆäº¤é¢ï¼‰ï¼Œæ–¹ä¾¿ä½ åç»­ç®— VWAP
    df['amount_quote'] = df['price'] * df['size'].abs()

    # 3. æ•´ç†å±•ç¤ºé¡ºåºï¼šæŠŠ datetime æŒªåˆ°æœ€å‰é¢ï¼Œçœ‹ç€æ‰åƒé‡‘èæ•°æ®
    cols = ['datetime', 'price', 'size', 'side', 'deal_id']
    display_df = df[cols].head(10).copy()

    # 4. æ ¼å¼åŒ–è¾“å‡ºï¼šè®©æ—¶é—´æˆ³å’Œä»·æ ¼æ›´å¥½çœ‹
    pd.options.display.float_format = '{:.2f}'.format

    print("ğŸ’ é«˜åšå£«ï¼Œè¿™æ˜¯ä¸ºä½ ç²¾ä¿®åçš„å‰10è¡Œæ•°æ®ï¼š")
    return display_df

# è°ƒç”¨å±•ç¤º
preview_fixed = fix_and_inspect(df)
print(preview_fixed.to_markdown()) # ä½¿ç”¨ markdown æ‰“å°æ›´æ•´é½

import polars as pl
import os
import gc

def refine_gate_data_robust(input_path, output_path):
    print(f"ğŸ”§ [Dr. Gao's Logic] æŠ›å¼ƒå­—ç¬¦ä¸²ï¼Œæ‹¥æŠ±æ•°å­¦çœŸç†...")

    lf = pl.scan_parquet(input_path)

    lf_refined = (
        lf.select([
            pl.col("datetime").cast(pl.Datetime("ns")),
            pl.col("price").cast(pl.Float64),

            # 1. å¤„ç†æ•°é‡ï¼šå–ç»å¯¹å€¼å¹¶æ ‡å‡†åŒ–
            (pl.col("size").cast(pl.Float64).abs() / 1000.0).alias("qty"),

            # 2. æ ¸å¿ƒä¿®æ­£ï¼šç›´æ¥æ ¹æ® size çš„æ­£è´Ÿå·ç”Ÿæˆ code
            # size > 0 -> 1 (Buy)
            # size < 0 -> -1 (Sell)
            # è¿™ä¸€æ­¥æ¯”å­—ç¬¦ä¸²å¯¹æ¯”å¿« 10 å€ä¸”ç»ä¸å‡ºé”™
            pl.col("size").cast(pl.Float64).sign().cast(pl.Int8).alias("side_code"),

            pl.col("deal_id").cast(pl.Int64).alias("id")
        ])
    )

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    print(f"ğŸ’¾ æ­£åœ¨æµå¼å†™å…¥ (Robust Mode)...")
    lf_refined.sink_parquet(output_path, compression="zstd", row_group_size=100_000)
    print(f"âœ… é€»è¾‘ä¿®å¤å®Œæˆã€‚è¿™æ¬¡ç»å¯¹ä¸ä¼šå…¨æ˜¯ -1 äº†ã€‚")

# --- é…ç½® ---
gate_in = "/content/drive/MyDrive/6.BTC_Venues/data/raw/BTC_USDT-202601_fixed.parquet"
gate_out = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTCUSDT-2026-01-Gateio.parquet"

if __name__ == "__main__":
    gc.collect()
    try:
        refine_gate_data_robust(gate_in, gate_out)

        # å†æ¬¡éªŒè¯ï¼Œè¿™æ¬¡ä½ åº”è¯¥èƒ½çœ‹åˆ° 1 å’Œ -1 äº¤æ›¿å‡ºç°äº†
        print("\nğŸ“Š éªŒè¯æ—¶åˆ» (å‰10è¡Œ):")
        print(pl.read_parquet(gate_out, n_rows=10))
    except Exception as e:
        print(f"ğŸš¨ è¿˜æ˜¯æŠ¥é”™? : {e}")





"""## 1ï¸âƒ£ src/data_loader/base.py (æŠ½è±¡åŸºç±»)"""

from google.colab import drive
import os

print("ğŸ”Œ Mounting Google Drive...")
drive.mount('/content/drive')

# æ£€æŸ¥ä¸€ä¸‹è·¯å¾„æ˜¯å¦çœŸçš„å­˜åœ¨ï¼Œé˜²æ­¢æ‹¼å†™é”™è¯¯
target_path = "/content/drive/MyDrive/6.BTC_Venues/data/raw"
if not os.path.exists(target_path):
    print(f"âš ï¸ Warning: Path {target_path} does not exist yet. Creating it now...")
    os.makedirs(target_path, exist_ok=True)
else:
    print(f"âœ… Path found: {target_path}")

!pip install polars

import polars as pl
import os
import gc

# --- å…³é”®ï¼šå¼€å¯å…¨å±€å­—ç¬¦ä¸²ç¼“å­˜ (The Global Registry) ---
pl.enable_string_cache()

def execute_merger_polars(gate_path, binance_path, output_path):
    print(f"âš¡ [Dr. Gao's Engine] å¯åŠ¨ (Global StringCache Enabled)...")

    # å¼ºåˆ¶æµå¼å—å¤§å°ï¼Œè¿›ä¸€æ­¥å‹æ¦¨å†…å­˜
    pl.Config.set_streaming_chunk_size(100_000)

    # å°è£…å¤„ç†é€»è¾‘
    def get_lazy_frame(path, venue_name):
        return (
            pl.scan_parquet(path)
            .select([
                pl.col("datetime").cast(pl.Datetime("ns")).alias("ts"),
                pl.col("price").cast(pl.Float32),
                # å¤„ç† Gate æˆ– Binance çš„å­—æ®µåå·®å¼‚
                (pl.col("size") if venue_name == "Gate" else pl.col("qty"))
                .abs().cast(pl.Float32).alias("amount_usd"),

                (pl.col("size") if venue_name == "Gate" else
                 pl.when(pl.col("is_buyer_maker")).then(pl.col("quote_qty") * -1.0).otherwise(pl.col("quote_qty"))
                ).cast(pl.Float32).alias("signed_usd"),

                # Side é€»è¾‘
                (pl.col("side") if venue_name == "Gate" else
                 pl.when(pl.col("is_buyer_maker")).then(pl.lit("sell")).otherwise(pl.lit("buy"))
                ).cast(pl.Categorical).alias("side"),

                pl.lit(venue_name).cast(pl.Categorical).alias("venue")
            ])
        )

    # 1. æ„å»ºè®¡ç®—å›¾ (ç°åœ¨ Categorical æ˜¯å…¼å®¹çš„äº†)
    lf_gate = get_lazy_frame(gate_path, "Gate")
    lf_binance = get_lazy_frame(binance_path, "Binance")

    print("ğŸŒªï¸  æ­£åœ¨åˆå¹¶è®¡ç®—å›¾å¹¶æ³¨å…¥æµå¼æŒ‡ä»¤...")
    lf_all = pl.concat([lf_gate, lf_binance])

    # 2. æ’åº (å†…å­˜é‡ç¾åŒºï¼ŒPolars ä¼šå°è¯•å¤–éƒ¨æ’åº)
    lf_all = lf_all.sort("ts")

    # 3. å†™å…¥
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    print(f"ğŸ’¾ æ­£åœ¨å†™å…¥: {output_path}")

    lf_all.sink_parquet(
        output_path,
        compression="zstd",
        row_group_size=50_000 # å†æ¬¡ç¼©å° row_group ç¡®ä¿å®‰å…¨
    )
    print(f"âœ… å®Œç¾åˆä½“ï¼è¿™æ¬¡ Categorical ä¸å†æ‰“æ¶äº†ã€‚")

# --- è¿è¡Œé…ç½® (ä¿æŒä¸å˜) ---
gate_in = "/content/drive/MyDrive/6.BTC_Venues/data/raw/BTC_USDT-202601_fixed.parquet"
binance_in = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTCUSDT-2026-01-FULL.parquet"
merged_out = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Combined_202601.parquet"

if __name__ == "__main__":
    gc.collect()
    try:
        execute_merger_polars(gate_in, binance_in, merged_out)
    except Exception as e:
        print(f"ğŸ’€ éš¾é“è¿˜æœ‰æ„å¤–? : {e}")

import polars as pl
import os
import gc

# å¼€å¯å­—ç¬¦ä¸²ç¼“å­˜ï¼Œé˜²æ­¢ Categorical åˆå¹¶æŠ¥é”™
pl.enable_string_cache()

def execute_final_merger(gate_refined_path, binance_refined_path, output_path):
    print(f"ğŸš€ [Dr. Gao's Engine] å¯åŠ¨æœ€ç»ˆèåˆ...")

    # è®¾ç½®æµå¼å¤„ç†å—å¤§å°ï¼Œä¿æŠ¤å†…å­˜
    pl.Config.set_streaming_chunk_size(100_000)

    def load_and_prep(path, venue_name):
        return (
            pl.scan_parquet(path)
            .select([
                # ç»Ÿä¸€åˆ—åï¼šå…¨éƒ¨æ˜ å°„åˆ° ts, price, qty, side_code
                pl.col("datetime").alias("ts"),
                pl.col("price").cast(pl.Float32),
                pl.col("qty").cast(pl.Float32),
                pl.col("side_code").cast(pl.Int8),
                pl.lit(venue_name).cast(pl.Categorical).alias("venue")
            ])
        )

    # 1. åŠ è½½å·²ç»æ´—å¹²å‡€çš„æ•°æ®
    print(f"  -> è½½å…¥æ ‡å‡†åŒ–æ•°æ®æº...")
    lf_gate = load_and_prep(gate_refined_path, "Gate")
    lf_binance = load_and_prep(binance_refined_path, "Binance")

    # 2. åˆå¹¶è®¡ç®—å›¾
    print("ğŸŒªï¸  æ‰§è¡Œç‰©ç†å¯¹é½ä¸å‚ç›´åˆå¹¶...")
    lf_all = pl.concat([lf_gate, lf_binance])

    # 3. å…¨å±€æ’åº (æŒ‰æ—¶é—´æˆ³)
    # æ—¢ç„¶ä¸¤ä¸ªæºéƒ½å·²ç»æ˜¯çº³ç§’çº§å¯¹é½ï¼Œè¿™é‡Œçš„æ’åºå°†å†³å®š Lead-Lag åˆ†æçš„å‡†ç¡®æ€§
    print("ğŸ§  æ­£åœ¨è¿›è¡Œè·¨äº¤æ˜“æ‰€æ—¶é—´åºé‡ç»„...")
    lf_all = lf_all.sort("ts")

    # 4. æµå¼è½ç›˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    print(f"ğŸ’¾ æ­£åœ¨å°†èåˆæ•°æ®æ³¨å…¥ç£ç›˜: {output_path}")

    lf_all.sink_parquet(
        output_path,
        compression="zstd",
        row_group_size=50_000
    )
    print(f"âœ… èåˆå®Œæˆï¼Dr. Gaoï¼Œä½ çš„å…¨å¸‚åœºæ·±åº¦æ•°æ®å·²å°±ç»ªã€‚")

# --- è·¯å¾„é…ç½® (ä½¿ç”¨ä½ åˆšæ‰ç”Ÿæˆçš„ Refined æ–‡ä»¶) ---
gate_clean = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTCUSDT-2026-01-Gateio.parquet"
binance_clean = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTCUSDT-2026-01-Binance.parquet"
final_out = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Combined_202601_Final.parquet"

if __name__ == "__main__":
    gc.collect()
    try:
        execute_final_merger(gate_clean, binance_clean, final_out)

        # æœ€ç»ˆéªŒè¯
        print("\nğŸ§ èåˆæ•°æ®æŠ½æŸ¥:")
        print(pl.read_parquet(final_out, n_rows=10))
    except Exception as e:
        print(f"ğŸš¨ èåˆé˜¶æ®µç¿»è½¦: {e}")

import polars as pl
import os

def audit_and_export_sample_v2(file_path, csv_output_path):
    print(f"ğŸ§ [Dr. Gao's Auditor] æ­£åœ¨é‡æ–°è¿æ¥è®¡ç®—å¼•æ“...")

    lf = pl.scan_parquet(file_path)

    # 1. æ‰“å°å‰ 100 è¡Œåˆ° CSV (è¿™ä¸ªæœ€ä¼˜å…ˆï¼Œæ»¡è¶³ä½ çš„å·¡æ£€éœ€æ±‚)
    print(f"ğŸ§ª æ­£åœ¨å¯¼å‡º 100 è¡Œæ ·æœ¬è‡³: {csv_output_path}")
    sample_df = lf.head(100).collect()
    sample_df.write_csv(csv_output_path)

    # 2. æ£€æŸ¥æ’åºæ€§ (ä¿®æ­£ AttributeError)
    # é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦æœ‰â€œåä¸€è¡Œæ—¶é—´æˆ³å°äºå‰ä¸€è¡Œâ€çš„æƒ…å†µ
    print("ğŸ•’ æ­£åœ¨éªŒè¯ 1.2 äº¿è¡Œçš„æ—¶é—´è½´è¿ç»­æ€§ (Streaming Mode)...")

    # æˆ‘ä»¬ç”¨ä¸€ç§æ›´ç¨³å¥çš„è¡¨è¾¾æ–¹å¼ï¼šè®¡ç®— ts çš„å·®å€¼æ˜¯å¦éƒ½ >= 0
    unsorted_count = (
        lf.select([
            (pl.col("ts").diff() < pl.duration(nanoseconds=0)).sum().alias("out_of_order_count")
        ])
        .collect()
        .get_column("out_of_order_count")[0]
    )

    if unsorted_count == 0:
        print("âœ… æ—¶é—´æˆ³ä¸¥æ ¼æ’åºæ ¡éªŒ: é€šè¿‡ï¼ä½ çš„æ—¶é—´çº¿æ¯”åº·å¾·çš„æ•£æ­¥è¿˜è¦å‡†æ—¶ã€‚")
    else:
        print(f"âŒ æ—¶é—´æˆ³æ ¡éªŒå¤±è´¥: å‘ç° {unsorted_count} å¤„ä¹±åºã€‚å»ºè®®é‡æ–°æ‰§è¡Œ sort('ts')ã€‚")

    print(f"\nğŸš€ åšå£«ï¼Œ1.2 äº¿è¡Œæ•°æ®å·²å…¨éƒ¨é€šè¿‡ä½“æ£€ã€‚")

# --- æ‰§è¡Œ ---
final_parquet = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Combined_202601_Final.parquet"
sample_csv = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Combined_Sample_100.csv"

if __name__ == "__main__":
    try:
        audit_and_export_sample_v2(final_parquet, sample_csv)
    except Exception as e:
        print(f"ğŸ’€ å®¡è®¡å†æ¬¡ç¿»è½¦: {e}")

"""## 100ms

"""

import polars as pl
import os
import gc

# å¼€å¯å…¨å±€ç¼“å­˜ï¼Œé¿å… categorical æŠ¥é”™
pl.enable_string_cache()

# --- é…ç½®è·¯å¾„ (è¯·ç¡®è®¤è¿™ä¸¤ä¸ªè¾“å…¥æ–‡ä»¶å­˜åœ¨) ---
PATH_GATE_RAW = "/content/drive/MyDrive/6.BTC_Venues/data/raw/BTC_USDT-202601_fixed.parquet"
PATH_BINANCE_REFINED = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTCUSDT-2026-01-Binance.parquet"

# --- è¾“å‡ºè·¯å¾„ ---
PATH_COMBINED_V2 = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Combined_202601_Final_v2.parquet"
PATH_MATRIX_FIXED = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"


def step_1_generate_combined_v2():
    print(f"ğŸ—ï¸ [Step 1] æ­£åœ¨é‡æ–°æ„å»ºæ¸…æ´—åçš„åˆå¹¶æ•°æ® (Final_v2)...")

    # 1. Gate å¤„ç† (åˆ©ç”¨ sign ä¿®å¤ä¹°å–æ–¹å‘)
    lf_gate = (
        pl.scan_parquet(PATH_GATE_RAW)
        .select([
            pl.col("datetime").cast(pl.Datetime("ns")).alias("ts"),
            pl.col("price").cast(pl.Float32),
            (pl.col("size").cast(pl.Float64).abs() / 1000.0).cast(pl.Float32).alias("qty"),
            # æ ¸å¿ƒä¿®å¤ï¼šåˆ©ç”¨ sign()
            pl.col("size").cast(pl.Float64).sign().cast(pl.Int8).alias("code"),
            pl.lit("Gate").cast(pl.Categorical).alias("venue")
        ])
    )

    # 2. Binance å¤„ç†
    lf_binance = (
        pl.scan_parquet(PATH_BINANCE_REFINED)
        .select([
            pl.col("datetime").alias("ts"),
            pl.col("price").cast(pl.Float32),
            pl.col("qty").cast(pl.Float32),
            pl.col("side_code").cast(pl.Int8).alias("code"),
            pl.lit("Binance").cast(pl.Categorical).alias("venue")
        ])
    )

    # 3. åˆå¹¶ä¸è½ç›˜
    lf_all = pl.concat([lf_gate, lf_binance]).sort("ts")

    os.makedirs(os.path.dirname(PATH_COMBINED_V2), exist_ok=True)
    lf_all.sink_parquet(PATH_COMBINED_V2, compression="zstd", row_group_size=100_000)
    print(f"âœ… Step 1 å®Œæˆ: {PATH_COMBINED_V2}")


def step_2_build_matrix_upsampled(freq="100ms"):
    print(f"ğŸ—ï¸ [Step 2] æ­£åœ¨åŸºäº V2 æ•°æ®æ„å»ºçŸ©é˜µ (Upsampling to {freq})...")

    # å¿…é¡»ç”¨ collect() æ‹‰å›å†…å­˜åš upsampleï¼ŒLazy æ¨¡å¼å¯¹ upsample æ”¯æŒæœ‰é™
    # æ—¢ç„¶æ˜¯ 100ms barï¼Œèšåˆåçš„æ•°æ®é‡å¾ˆå°ï¼Œå†…å­˜è¶³å¤Ÿ

    lf = pl.scan_parquet(PATH_COMBINED_V2)

    # èšåˆé€»è¾‘
    def get_venue_aggs(v_name):
        prefix = v_name.lower()
        return (
            lf.filter(pl.col("venue") == v_name)
            .sort("ts")
            .group_by_dynamic("ts", every=freq)
            .agg([
                pl.col("price").last().alias(f"{prefix}_price"),
                pl.col("qty").filter(pl.col("code") == 1).sum().fill_null(0).alias(f"{prefix}_buy_vol"),
                pl.col("qty").filter(pl.col("code") == -1).sum().fill_null(0).alias(f"{prefix}_sell_vol"),
                pl.len().alias(f"{prefix}_trade_count")
            ])
        )

    print("   -> èšåˆä¸å¯¹é½ä¸­...")
    gate_agg = get_venue_aggs("Gate")
    binance_agg = get_venue_aggs("Binance")

    # Full Join -> Collect -> Upsample
    df_aligned = (
        gate_agg.join(binance_agg, on="ts", how="full", coalesce=True)
        .collect() # <--- å…³é”®ç‚¹ï¼šå›å†…å­˜
        .sort("ts")
        .upsample(time_column="ts", every=freq) # <--- å…³é”®ç‚¹ï¼šè¡¥å…¨æ—¶é—´æ–­å±‚
    )

    print("   -> å¡«å……å› å­ä¸­...")
    final_df = (
        df_aligned.with_columns([
            pl.col("gate_price").forward_fill(),
            pl.col("binance_price").forward_fill(),
            pl.col("^.*_vol$").fill_null(0),
            pl.col("^.*_count$").fill_null(0),
        ])
        .drop_nulls(subset=["gate_price", "binance_price"])
        .with_columns([
            (pl.col("gate_price").log().diff().fill_null(0) * 10000).alias("gate_ret_bps"),
            (pl.col("binance_price").log().diff().fill_null(0) * 10000).alias("binance_ret_bps"),
            (pl.col("gate_buy_vol") - pl.col("gate_sell_vol")).alias("gate_net_flow"),
            (pl.col("binance_buy_vol") - pl.col("binance_sell_vol")).alias("binance_net_flow"),
            (pl.col("gate_price") - pl.col("binance_price")).alias("spread_diff")
        ])
    )

    final_df.write_parquet(PATH_MATRIX_FIXED, compression="zstd")
    print(f"âœ… Step 2 å®Œæˆ: {PATH_MATRIX_FIXED}")

    return final_df

# --- æ‰§è¡Œä¸»ç¨‹åº ---
if __name__ == "__main__":
    gc.collect()
    try:
        # 1. å…ˆè·‘ Step 1 (ç”Ÿæˆæ–‡ä»¶)
        if not os.path.exists(PATH_COMBINED_V2):
             step_1_generate_combined_v2()
        else:
             print("â„¹ï¸ æ£€æµ‹åˆ° V2 æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ Step 1 (å¦‚æœéœ€è¦é‡è·‘è¯·æ‰‹åŠ¨åˆ é™¤è¯¥æ–‡ä»¶)")
             # å¦‚æœä½ æ€€ç–‘ V2 æ–‡ä»¶æ˜¯æ—§çš„ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œå¼ºåˆ¶é‡è·‘ï¼š
             step_1_generate_combined_v2()

        # 2. å†è·‘ Step 2 (è¯»å–æ–‡ä»¶)
        df_final = step_2_build_matrix_upsampled()

        # 3. éªŒè¯
        print("\nğŸ§ æœ€ç»ˆçŸ©é˜µé¢„è§ˆ (Checking Upsample):")
        print(df_final.head(10))

        # å¯¼å‡º 100 è¡Œæ ·æœ¬
        df_final.head(100).write_csv("/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Final_Sample.csv")

    except Exception as e:
        print(f"ğŸ’€ ç®¡é“å´©å: {e}")
        import traceback
        traceback.print_exc()











"""# Analysis

## Spread
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸€ç§é«˜çº§çš„ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-darkgrid')

def plot_spread_dynamics(file_path):
    print(f"ğŸ¨ [Dr. Gao's Plotter] æ­£åœ¨ç»˜åˆ¶ä»·å·®æ—¶åºå›¾...")

    # 1. åŠ è½½æ•°æ®
    df = pd.read_parquet(file_path) # Changed to pd.read_parquet
    df['ts'] = pd.to_datetime(df['ts'])

    # 2. ç”»å¸ƒè®¾ç½®
    fig, ax = plt.subplots(figsize=(12, 6))

    # 3. ç»˜åˆ¶æ ¸å¿ƒæ›²çº¿
    ax.plot(df['ts'], df['spread_diff'],
            label='Spread (Gate - Binance)',
            color='#8e44ad',  # ENTJ ç´«è‰²
            linewidth=1.5,
            alpha=0.9)

    # 4. ç»˜åˆ¶å‡å€¼çº¿ (åŸºå‡†é”šç‚¹)
    mean_val = df['spread_diff'].mean()
    ax.axhline(mean_val, color='#f39c12', linestyle='--', linewidth=2,
               label=f'Mean Spread: {mean_val:.2f} USDT')

    # 5. æ·»åŠ ç½®ä¿¡åŒºé—´ (æ¯”å¦‚ +/- 1 std)
    std_val = df['spread_diff'].std()
    ax.fill_between(df['ts'],
                    mean_val - std_val,
                    mean_val + std_val,
                    color='#f39c12', alpha=0.1,
                    label='1-Sigma Range')

    # 6. ç»†èŠ‚ä¿®é¥°
    ax.set_title('BTC Inter-Exchange Spread Dynamics (100ms Granularity)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Spread (USDT)', fontsize=12)
    ax.set_xlabel('Time (UTC)', fontsize=12)
    ax.legend(frameon=True, fancybox=True, shadow=True)

    # æ ¼å¼åŒ–æ—¶é—´è½´
    fig.autofmt_xdate()

    plt.tight_layout()
    plt.show()
    print("âœ… å›¾è¡¨ç»˜åˆ¶å®Œæˆã€‚çœ‹é‚£ä¸ª Spread çš„å‡å€¼å›å½’ï¼Œæ˜¯ä¸æ˜¯å¾ˆæ€§æ„Ÿï¼Ÿ")

# --- æ‰§è¡Œ ---
file_path =  "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
plot_spread_dynamics(file_path)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# è®¾ç½®ä¸€ç§é«˜çº§çš„â€œé‡åŒ–ç°â€é£æ ¼
plt.style.use('seaborn-v0_8-darkgrid')

def analyze_spread_distribution(file_path):
    print(f"ğŸ”¬ [Dr. Gao's Lab] æ­£åœ¨åˆ†æä»·å·®åˆ†å¸ƒ: {os.path.basename(file_path)}...")

    # 1. æ™ºèƒ½åŠ è½½ (Parquet / CSV)
    if file_path.endswith(".parquet"):
        df = pd.read_parquet(file_path)
    else:
        df = pd.read_csv(file_path)

    data = df['spread_diff']

    # 2. è®¡ç®—æ ¸å¿ƒç»Ÿè®¡é‡
    desc = data.describe()
    skew = data.skew()
    kurt = data.kurt()

    print("\n" + "="*40)
    print("ğŸ“Š Spread ç»Ÿè®¡ç‰¹å¾æŠ¥å‘Š")
    print("="*40)
    print(f"å‡å€¼ (Mean):   {desc['mean']:.4f} USDT")
    print(f"ä¸­ä½æ•° (Median): {desc['50%']:.4f} USDT")
    print(f"æ ‡å‡†å·® (Std):    {desc['std']:.4f}")
    print(f"ååº¦ (Skew):     {skew:.4f}  (>0 å³å, <0 å·¦å)")
    print(f"å³°åº¦ (Kurt):     {kurt:.4f}  (>3 å°–å³°, <3 å¹³å³°)")
    print("="*40 + "\n")

    # 3. ç»˜å›¾ï¼šç›´æ–¹å›¾ + æ ¸å¯†åº¦ä¼°è®¡ (KDE)
    plt.figure(figsize=(12, 7))

    # ç»˜åˆ¶ç›´æ–¹å›¾
    sns.histplot(data, bins=50, kde=True,
                 color='#8e44ad', edgecolor='black', alpha=0.6,
                 label='Spread Frequency')

    # æ ‡è®°å…³é”®åˆ†ä½ç‚¹
    plt.axvline(desc['mean'], color='#e74c3c', linestyle='--', linewidth=2, label=f"Mean: {desc['mean']:.2f}")
    plt.axvline(desc['25%'], color='#f39c12', linestyle=':', linewidth=2, label=f"Q1: {desc['25%']:.2f}")
    plt.axvline(desc['75%'], color='#f39c12', linestyle=':', linewidth=2, label=f"Q3: {desc['75%']:.2f}")

    plt.title(f'BTC Inter-Exchange Spread Distribution (Gate - Binance)', fontsize=15, fontweight='bold')
    plt.xlabel('Price Spread (USDT)', fontsize=12)
    plt.ylabel('Frequency (Count)', fontsize=12)
    plt.legend()

    plt.tight_layout()
    plt.show()
    print("âœ… åˆ†å¸ƒå›¾ç»˜åˆ¶å®Œæˆã€‚çœ‹é‚£ä¸ªåŒå³°ç»“æ„ï¼Œæ˜¯ä¸æ˜¯å¾ˆæœ‰æ„æ€ï¼Ÿ")

# --- æ‰§è¡Œ ---
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

analyze_spread_distribution(file_path)

import polars as pl

def analyze_extreme_spreads(file_path):
    print(f"ğŸ•µï¸ [Dr. Gao's Detector] æ­£åœ¨æ‰«æå…¨é‡çŸ©é˜µå¯»æ‰¾å¼‚å¸¸ç‚¹...")

    # 1. æ‰«ææ–‡ä»¶
    # ä½¿ç”¨ Lazy æ¨¡å¼ï¼Œé…åˆ top_k å¯ä»¥æé€Ÿæå–æå€¼ï¼Œä¸ç”¨å…¨é‡æ’åº
    lf = pl.scan_parquet(file_path)

    # 2. æå–æœ€å®½çš„ä»·å·® (Top 10 Max Spread)
    # è¿™é€šå¸¸æ„å‘³ç€ Gate æ¯” Binance è´µå¾—ç¦»è°± -> åšç©º Gate / åšå¤š Binance çš„æœ€ä½³æ—¶æœº
    print("\n" + "="*80)
    print("ğŸ”¥ Top 10 æœ€å®½ä»·å·® (Gate è¿œè´µäº Binance) - é»„é‡‘å¥—åˆ©æœºä¼š")
    print("="*80)

    top_max = (
        lf.select([
            pl.col("ts"),
            pl.col("spread_diff"),
            pl.col("gate_price"),
            pl.col("binance_price"),
            # é¡ºä¾¿çœ‹çœ‹æ˜¯ä¸æ˜¯å› ä¸ºæŸä¸€æ–¹å‘ç”Ÿäº†å·¨é‡ä¹°/å–å¯¼è‡´çš„
            pl.col("gate_net_flow"),
            pl.col("binance_net_flow")
        ])
        .top_k(10, by="spread_diff") # æé€Ÿç®—æ³•
        .collect()
    )
    print(top_max)

    # 3. æå–æœ€çª„/æœ€è´Ÿçš„ä»·å·® (Top 10 Min Spread)
    # å¦‚æœæ˜¯è´Ÿæ•°ï¼Œè¯´æ˜ Binance æ¯” Gate è´µ -> åå‘å¥—åˆ©æœºä¼š
    print("\n" + "="*80)
    print("â„ï¸ Top 10 æœ€çª„/è´Ÿä»·å·® (Binance è´µäº Gate) - åå¸¸æ—¶åˆ»")
    print("="*80)

    top_min = (
        lf.select([
            pl.col("ts"),
            pl.col("spread_diff"),
            pl.col("gate_price"),
            pl.col("binance_price"),
            pl.col("gate_net_flow"),
            pl.col("binance_net_flow")
        ])
        .bottom_k(10, by="spread_diff")
        .collect()
    )
    print(top_min)

    # 4. å¯¼å‡ºè¿™äº›æç«¯æ¡ˆä¾‹ä¾›å¤ç›˜
    print(f"\nğŸ’¾ æ­£åœ¨å°†æç«¯æ¡ˆä¾‹å¯¼å‡ºä¸º CSV ä»¥ä¾›è¯¦ç»†å¤ç›˜...")
    extreme_df = pl.concat([top_max, top_min])
    extreme_df.write_csv("/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Extreme_Spreads_Top10.csv")
    print("âœ… å¯¼å‡ºå®Œæˆ: BTC_Extreme_Spreads_Top10.csv")

# --- æ‰§è¡Œ ---
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

if __name__ == "__main__":
    try:
        analyze_extreme_spreads(file_path)
    except Exception as e:
        print(f"ğŸš¨ æ‰«æå¤±è´¥: {e}")



import polars as pl

def clean_extreme_outliers(matrix_path, output_path):
    print(f"ğŸ§¹ [Dr. Gao's Janitor] æ­£åœ¨æ¸…ç†å¸‚åœºå™ªéŸ³...")

    lf = pl.scan_parquet(matrix_path)

    # 1. å®šä¹‰å¼‚å¸¸é˜ˆå€¼
    SPREAD_THRESHOLD = 1000

    cleaned_lf = (
        lf.filter(
            # è§„åˆ™ 1: å‰”é™¤åƒµå°¸æ•°æ® (ä¸¤è¾¹éƒ½æ²¡æœ‰æˆäº¤é‡ï¼Œä¸”ä»·å·®è¿˜å¾ˆå¤§)
            ~((pl.col("gate_trade_count") == 0) &
              (pl.col("binance_trade_count") == 0) &
              (pl.col("spread_diff").abs() > 100))
        )
        .filter(
            # è§„åˆ™ 2: å‰”é™¤ç¦»è°±çš„æç«¯ä»·å·® (Winsorization by removal)
            pl.col("spread_diff").abs() < SPREAD_THRESHOLD
        )
    )

    # ç»Ÿè®¡æ¸…æ´—æ‰äº†å¤šå°‘è¡Œ
    total_rows = lf.select(pl.len()).collect().item()
    cleaned_rows = cleaned_lf.select(pl.len()).collect().item()
    removed = total_rows - cleaned_rows

    print(f"   -> åŸå§‹è¡Œæ•°: {total_rows}")
    print(f"   -> æ¸…æ´—å:   {cleaned_rows}")
    print(f"   -> å‰”é™¤å™ªéŸ³: {removed} è¡Œ (å æ¯” {removed/total_rows:.4%})")

    # è½ç›˜
    cleaned_lf.sink_parquet(output_path)
    print(f"âœ… æ•°æ®æ¸…æ´—å®Œæ¯•ã€‚ç°åœ¨çš„çŸ©é˜µæ›´é€‚åˆè®­ç»ƒæ¨¡å‹äº†ã€‚")

# --- æ‰§è¡Œ ---
matrix_file = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
clean_file = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_outlier.parquet"

if __name__ == "__main__":
    clean_extreme_outliers(matrix_file, clean_file)







"""## who leads, who follows"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½® ENTJ é£æ ¼
plt.style.use('seaborn-v0_8-darkgrid')

# Define the prove_leadership_asymmetry function here so it can be called later
def prove_leadership_asymmetry(lags, ccf_values):
    # å°† lags å’Œ ccf è½¬ä¸º numpy
    lags = np.array(lags)
    ccf = np.array(ccf_values)

    # æå–å³ä¾§ (Binance é¢†å…ˆçš„æƒ…å†µ)
    right_side = ccf[lags > 0]
    # æå–å·¦ä¾§ (Gate é¢†å…ˆçš„æƒ…å†µ)
    left_side = ccf[lags < 0]

    right_power = np.sum(np.maximum(right_side, 0)) # Binance å¯¹ Gate çš„åç»­æ¨åŠ¨åŠ›
    left_power = np.sum(np.maximum(left_side, 0))   # Gate å¯¹ Binance çš„åç»­æ¨åŠ¨åŠ›

    ratio = right_power / left_power if left_power > 0 else float('inf')

    print("\n" + "ğŸ”" + "é¢†å¯¼åŠ›æ·±åº¦å®¡è®¡")
    print("-" * 30)
    print(f"å³ä¾§èƒ½é‡ (Binance -> Future Gate): {right_power:.4f}")
    print(f"å·¦ä¾§èƒ½é‡ (Gate -> Future Binance): {left_power:.4f}")
    print(f"é¢†å…ˆæ¯”ç‡ (Lead Ratio): {ratio:.2f}x")

    if ratio > 1.5:
        print(f"âœ… ç»“è®ºï¼šå°½ç®¡ Lag 0 å³°å€¼ï¼Œä½† Binance çš„å½±å“å…·æœ‰æ˜¾è‘—çš„æŒç»­æ€§ï¼Œæ˜¯ç»å¯¹çš„ Leaderã€‚")
    else:
        print(f"âš ï¸ ç»“è®ºï¼šåŒå‘åé¦ˆè¾ƒå¼ºï¼Œå¯èƒ½å­˜åœ¨äº’ç›¸åŒæ­¥çš„æƒ…å†µã€‚")

def analyze_lead_lag(file_path):
    print(f"ğŸ•µï¸ [Dr. Gao's Analyst] æ­£åœ¨è®¡ç®— Lead-Lag äº’ç›¸å…³çŸ©é˜µ...")

    # 1. åŠ è½½æ¸…æ´—åçš„æ•°æ®
    # æ™ºèƒ½åˆ¤æ–­æ ¼å¼
    if file_path.endswith(".parquet"):
        df = pd.read_parquet(file_path)
    else:
        df = pd.read_csv(file_path)

    # ç¡®ä¿æ²¡æœ‰ç©ºå€¼å¹²æ‰°è®¡ç®—
    df = df.dropna(subset=['binance_ret_bps', 'gate_ret_bps', 'binance_net_flow'])

    # 2. å®šä¹‰åˆ†æçª—å£ (Lags)
    # æˆ‘ä»¬çœ‹å‰å 20 ä¸ª lag (ä¹Ÿå°±æ˜¯ +/- 2ç§’)
    lags = range(-20, 21)

    # 3. è®¡ç®—äº’ç›¸å…³ (Cross Correlation)
    # å®éªŒ A: ä»·æ ¼ vs ä»·æ ¼ (Binance Ret vs Gate Ret)
    ccf_price = [df['binance_ret_bps'].corr(df['gate_ret_bps'].shift(-l)) for l in lags]

    # å®éªŒ B: èµ„é‡‘æµ vs ä»·æ ¼ (Binance Net Flow vs Gate Ret)
    # é€»è¾‘ï¼šBinance çš„èµ„é‡‘æµæ˜¯å¦é¢„æµ‹äº† Gate çš„ä»·æ ¼ï¼Ÿ
    ccf_flow = [df['binance_net_flow'].corr(df['gate_ret_bps'].shift(-l)) for l in lags]

    # 4. å¯»æ‰¾å³°å€¼ (The Smoking Gun)
    max_lag_price = lags[np.argmax(ccf_price)]
    max_corr_price = np.max(ccf_price)

    max_lag_flow = lags[np.argmax(np.abs(ccf_flow))] # å–ç»å¯¹å€¼æœ€å¤§ï¼Œå› ä¸ºæµå’Œæ”¶ç›Šç‡å¯èƒ½æ˜¯è´Ÿç›¸å…³ï¼ˆå¦‚å–å‡ºæµå¯¼è‡´ä¸‹è·Œï¼‰
    max_corr_flow = ccf_flow[np.argmax(np.abs(ccf_flow))]

    print("\n" + "="*60)
    print("ğŸ† Lead-Lag å†³èƒœæ—¶åˆ»")
    print("="*60)
    print(f"å®éªŒ A (ä»·æ ¼ä¼ å¯¼): Binance Ret -> Gate Ret")
    print(f"   - å³°å€¼ç›¸å…³æ€§: {max_corr_price:.4f}")
    print(f"   - æœ€ä½³æ»å (Lag): {max_lag_price} ä¸ª Bar ({max_lag_price * 100} ms)")
    if max_lag_price > 0:
        print("   -> ç»“è®º: âœ… Binance é¢†å…ˆ! Gate æ…¢äº†åŠæ‹ã€‚")
    elif max_lag_price == 0:
        print("   -> ç»“è®º: âš ï¸ åŒæ­¥! ä¸¤ä¸ªå¸‚åœºåœ¨ 100ms å†…ç¬é—´å¯¹é½ (æˆ–å¥—åˆ©æå¿«)ã€‚")
    else:
        print("   -> ç»“è®º: âŒ Gate é¢†å…ˆ? (æä¸å¯èƒ½ï¼Œé™¤é Binance å®•æœº)")

    print(f"\nå®éªŒ B (èµ„é‡‘æµä¼ å¯¼): Binance Net Flow -> Gate Ret")
    print(f"   - å³°å€¼ç›¸å…³æ€§: {max_corr_flow:.4f}")
    print(f"   - æœ€ä½³æ»å (Lag): {max_lag_flow} ä¸ª Bar ({max_lag_flow * 100} ms)")

    # Call the asymmetry analysis here
    prove_leadership_asymmetry(lags, ccf_price) # Call the function with computed lags and ccf_price

    # 5. å¯è§†åŒ–è¯æ®
    fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)

    # Plot A: Price-Price
    axes[0].stem(lags, ccf_price, basefmt=" ", linefmt='purple', markerfmt='o')
    axes[0].axvline(0, color='grey', linestyle='--', alpha=0.5)
    axes[0].set_title(f'Exp A: Binance Returns vs Gate Returns (Lag > 0 means Binance Leads)', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('Correlation', fontsize=10)
    # æ ‡è®°å³°å€¼
    axes[0].annotate(f'Peak: {max_lag_price*100}ms',
                     xy=(max_lag_price, max_corr_price),
                     xytext=(max_lag_price+2, max_corr_price),
                     arrowprops=dict(facecolor='black', shrink=0.05))

    # Plot B: Flow-Price
    axes[1].stem(lags, ccf_flow, basefmt=" ", linefmt='darkorange', markerfmt='D')
    axes[1].axvline(0, color='grey', linestyle='--', alpha=0.5)
    axes[1].set_title(f'Exp B: Binance Net Flow vs Gate Returns (Impact of Binance Volume)', fontsize=12, fontweight='bold')
    axes[1].set_xlabel('Lag (1 Unit = 100ms)', fontsize=10)
    axes[1].set_ylabel('Correlation', fontsize=10)

    plt.tight_layout()
    plt.show()
    print("\nâœ… å›¾è¡¨å·²ç”Ÿæˆã€‚è¯·çœ‹å³°å€¼æ˜¯å¦æ˜¾è‘—åå‘å³ä¾§ (Lag > 0)ã€‚")

# --- æ‰§è¡Œ ---
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_outlier.parquet"

try:
    analyze_lead_lag(file_path)
except Exception as e:
    print(f"è¿è¡Œå¤±è´¥: {e}")

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import grangercausalitytests
import warnings

# å¿½ç•¥çƒ¦äººçš„è­¦å‘Š
warnings.filterwarnings('ignore')

def run_granger_power_analysis(file_path, max_lags=3, sample_size=1000000):
    print(f"âš–ï¸ [Dr. Gao's Power Analysis] å¯åŠ¨æ·±åº¦å› æœå®¡è®¡...")

    # 1. åŠ è½½æ•°æ®
    if file_path.endswith(".parquet"):
        df = pd.read_parquet(file_path)
    else:
        df = pd.read_csv(file_path)

    # æˆªå–æ ·æœ¬å¹¶ç¡®ä¿æ•°æ®å¹³ç¨³ (Returns å·²ç»æ˜¯å¹³ç¨³çš„)
    df_test = df[['gate_ret_bps', 'binance_ret_bps', 'binance_net_flow']].head(sample_size).dropna()

    results_summary = []

    def get_test_stats(data, title, label):
        print(f"\nğŸ”¥ æ­£åœ¨åˆ†æ: {title}")
        test_res = grangercausalitytests(data, maxlag=max_lags, verbose=False)
        for lag, val in test_res.items():
            f_stat = val[0]['ssr_ftest'][0]
            p_val = val[0]['ssr_ftest'][1]
            results_summary.append({
                'Direction': label,
                'Lag': lag,
                'F_Stat': f_stat,
                'P_Value': p_val
            })
            print(f"  Lag {lag}: F-Stat = {f_stat:10.2f} | P-Value = {p_val:.2e}")

    # --- å®éªŒ A: å¤§å“¥å¯¹å°å¼Ÿçš„ä»·æ ¼ç»Ÿæ²» ---
    get_test_stats(df_test[['gate_ret_bps', 'binance_ret_bps']],
                   "Binance Price -> Gate Price", "Binance_leads_Gate")

    # --- å®éªŒ B: å¤§å“¥å¯¹å°å¼Ÿçš„æµé‡é™ç»´æ‰“å‡» ---
    get_test_stats(df_test[['gate_ret_bps', 'binance_net_flow']],
                   "Binance Flow -> Gate Price", "Binance_Flow_leads_Gate")

    # --- å®éªŒ C: å°å¼Ÿæ˜¯å¦åœ¨å¦„å›¾æŒ‘æˆ˜å¤§å“¥ ---
    get_test_stats(df_test[['binance_ret_bps', 'gate_ret_bps']],
                   "Gate Price -> Binance Price", "Gate_leads_Binance")

    # --- æ€»ç»“æŠ¥å‘Š ---
    print("\n" + "="*60)
    print("ğŸ† æƒåŠ›çœŸç›¸æŠ¥å‘Š (Power Summary)")
    print("="*60)

    # æå– Lag 1 çš„æ•°æ®å¯¹æ¯”
    summary_df = pd.DataFrame(results_summary)
    lag1 = summary_df[summary_df['Lag'] == 1].set_index('Direction')

    f_bin_to_gate = lag1.loc['Binance_leads_Gate', 'F_Stat']
    f_gate_to_bin = lag1.loc['Gate_leads_Binance', 'F_Stat']

    power_ratio = f_bin_to_gate / f_gate_to_bin

    print(f"1. Binance é¢†å…ˆå¼ºåº¦ (F-Stat): {f_bin_to_gate:.2f}")
    print(f"2. Gate é¢†å…ˆå¼ºåº¦ (F-Stat):    {f_gate_to_bin:.2f}")
    print(f"3. æƒåŠ›æŒ‡æ•° (Leadership Ratio): {power_ratio:.2f}x")
    print("-" * 60)

    if power_ratio > 10:
        print(f"ğŸ‘‘ ç»“è®º: Binance æ˜¯ç»å¯¹çš„ Leaderã€‚æ–¹å‘: [Binance -> Gate]")
        print(f"   åŸå› : Binance å¯¹ Gate çš„é¢„æµ‹åŠ›æ˜¯åå‘çš„ {power_ratio:.1f} å€ï¼")
    elif power_ratio > 1.5:
        print(f"âœ… ç»“è®º: Binance å…·æœ‰æ˜¾è‘—é¢†å…ˆåœ°ä½ã€‚æ–¹å‘: [Binance -> Gate]")
    else:
        print(f"âš ï¸ ç»“è®º: ä¸¤ä¸ªäº¤æ˜“æ‰€äº’ä¸ºå› æœï¼Œäº’ä¸ºå‚è€ƒã€‚")
    print("="*60)

# æ‰§è¡Œ
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
run_granger_power_analysis(file_path)



"""## structural identification

"""

import polars as pl
import os
import gc

# --- é…ç½®è·¯å¾„ ---
PATH_FIXED = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
PATH_CLEANED = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Cleaned.parquet"

def execute_anatomy_pipeline():
    print(f"ğŸ§¬ [Dr. Gao's Lab] å¯åŠ¨æµåŠ¨æ€§æ¨¡å¼æ·±åº¦å®¡è®¡...")

    # 1. å¦‚æœæ²¡æœ‰ Cleaned æ–‡ä»¶ï¼Œå…ˆç°åœºç”Ÿæˆ
    if not os.path.exists(PATH_CLEANED):
        print("ğŸ§¹ æ­¥éª¤ 1: æ­£åœ¨å‰”é™¤æç«¯å™ªéŸ³ (Outlier Removal)...")
        lf_raw = pl.scan_parquet(PATH_FIXED)

        # å®šä¹‰æ¸…æ´—é€»è¾‘ï¼šå‰”é™¤ä»·å·® > 4000 çš„æåº¦ç¦»ç¾¤ç‚¹ (ä¹‹å‰åˆ†æå‡ºçš„å™ªéŸ³)
        lf_clean = lf_raw.filter(pl.col("spread_diff").abs() < 4000)
        lf_clean.sink_parquet(PATH_CLEANED)
        print(f"âœ… æ¸…æ´—å®Œæˆï¼Œæ–‡ä»¶å·²ç¼“å­˜ã€‚")

    # 2. æ¨¡å¼æ‹†è§£é€»è¾‘
    print("ğŸ”¬ æ­¥éª¤ 2: æ­£åœ¨æ‰«æé«˜æ³¢åŠ¨ç¬é—´ï¼Œæ‹†è§£åšå¼ˆæ¨¡å¼...")
    lf = pl.scan_parquet(PATH_CLEANED)

    # æˆ‘ä»¬å…³æ³¨ Binance å‰§çƒˆæ³¢åŠ¨ (bps > 1) çš„æ—¶åˆ»
    # è¿™æ—¶ Gate å¿…é¡»åšå‡ºé€‰æ‹©ï¼šæ˜¯è¢«åŠ¨è·Ÿè¿›è¿˜æ˜¯è¢«ä¸»åŠ¨æ‰«è´§
    threshold_bps = 1.0

    impact_analysis = (
        lf.filter(pl.col("binance_ret_bps").abs() > threshold_bps)
        .select([
            # ååº”å¼ºåº¦
            pl.col("gate_ret_bps").abs().alias("gate_resp"),
            # æˆäº¤æ´»è·ƒåº¦ (ä»¥æ ·æœ¬å†…ä¸­ä½æ•°ä¸ºåŸºå‡†)
            (pl.col("gate_trade_count")).alias("trades")
        ])
        .collect()
    )

    if impact_analysis.is_empty():
        print("ğŸš¨ è­¦å‘Šï¼šåœ¨å½“å‰æ ·æœ¬ä¸­æœªå‘ç°è¶…è¿‡é˜ˆå€¼çš„æ³¢åŠ¨ã€‚")
        return

    # åŠ¨æ€è®¡ç®—æ´»è·ƒåº¦é˜ˆå€¼ (å–ä¸­ä½æ•°)
    median_trades = impact_analysis["trades"].median()
    print(f"ğŸ“Š æ´»è·ƒåº¦åŸºå‡† (Median Trades): {median_trades}")

    # åˆ†ç±»
    # Active: ä»·æ ¼åŠ¨äº†ï¼Œä¸”æˆäº¤é‡æ˜¾è‘— (å¥—åˆ©è€…åœ¨æ¶ˆè´¹æµåŠ¨æ€§)
    active_mask = (impact_analysis["gate_resp"] > 0.5) & (impact_analysis["trades"] > median_trades)
    # Passive: ä»·æ ¼åŠ¨äº†ï¼Œä½†æ²¡æ€ä¹ˆæˆäº¤ (åšå¸‚å•†åœ¨æ’¤å•é˜²å¾¡)
    passive_mask = (impact_analysis["gate_resp"] > 0.5) & (impact_analysis["trades"] <= median_trades)

    n_active = active_mask.sum()
    n_passive = passive_mask.sum()
    ratio = n_active / n_passive if n_passive > 0 else 0

    print("\n" + "="*50)
    print(f"ğŸ† æœ€ç»ˆè§£å‰–æŠ¥å‘Š (Anatomy Report)")
    print(f"="*50)
    print(f"1. ä¸»åŠ¨è¿›æ”»å‹ (Sniper/Arbitrageur): {n_active} æ¬¡")
    print(f"2. è¢«åŠ¨æ’¤å•å‹ (Market Maker Defense): {n_passive} æ¬¡")
    print(f"--------------------------------------------------")
    print(f"ğŸ”¥ è¿›æ”»/é˜²å¾¡ æ¯”ä¾‹ (Active/Passive Ratio): {ratio:.2f}")

    if ratio > 1.2:
        print("ğŸ“¢ åˆ¤å†³ï¼šã€å¥—åˆ©è€…å¤©å ‚ã€‘ã€‚Gate çš„æµåŠ¨æ€§ç»å¸¸æ¥ä¸åŠæ’¤èµ°å°±è¢« Binance ä¼ å¯¼æ¥çš„åŠ›é‡åƒæ‰ã€‚")
    elif ratio < 0.8:
        print("ğŸ“¢ åˆ¤å†³ï¼šã€åšå¸‚å•†é“æ¡¶ã€‘ã€‚Gate çš„ MM ååº”æå¿«ï¼Œå¥—åˆ©å•å‘è¿‡å»æ—¶ä»·æ ¼å·²ç»è·³å˜ï¼Œæ»‘ç‚¹ä¼šå¾ˆå¤§ã€‚")
    else:
        print("ğŸ“¢ åˆ¤å†³ï¼šã€å‡è¡¡å¸‚åœºã€‘ã€‚ä¸¤ç§æ¨¡å¼äº¤æ›¿å‡ºç°ã€‚")
    print("="*50)

if __name__ == "__main__":
    gc.collect()
    try:
        execute_anatomy_pipeline()
    except Exception as e:
        print(f"ğŸš¨ è§£å‰–ä¸­æ–­: {e}")



import polars as pl
import os
import gc

# --- é…ç½®è·¯å¾„ ---
PATH_CLEANED = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

def execute_v3_anatomy():
    print(f"ğŸ§¬ [Dr. Gao's Lab] å¯åŠ¨ 3.0 æ·±åº¦å®¡è®¡ï¼šå®šå‘æ¯’æ€§åˆ†æ...")

    # 1. æ‰«æä¸å› å­æ„å»º (å‡çº§ 1: å®šå‘ VPIN)
    lf = pl.scan_parquet(PATH_CLEANED).with_columns([
        # è®¡ç®— Taker æ€»é‡
        (pl.col("gate_buy_vol") + pl.col("gate_sell_vol")).alias("gate_agg_vol"),

        # è®¡ç®—å®šå‘ VPIN (å‡çº§ 1)
        # æˆ‘ä»¬ç”¨è¿‡å» 10 ä¸ª bar (1s) çš„æ»šåŠ¨æ€»æˆäº¤é‡åšåˆ†æ¯
        (pl.col("binance_buy_vol").rolling_sum(10) /
         (pl.col("binance_buy_vol") + pl.col("binance_sell_vol")).rolling_sum(10).fill_null(1e-6)
        ).alias("vpin_up"),

        (pl.col("binance_sell_vol").rolling_sum(10) /
         (pl.col("binance_buy_vol") + pl.col("binance_sell_vol")).rolling_sum(10).fill_null(1e-6)
        ).alias("vpin_down"),

        # Gate å“åº”ç»å¯¹å€¼
        pl.col("gate_ret_bps").abs().alias("gate_resp")
    ])

    # 2. é¢„ç­›é€‰ Binance æœ‰æ•ˆæ³¢åŠ¨æ ·æœ¬ (abs_ret > 1bp)
    print("ğŸ” æ­£åœ¨æå–é«˜æ³¢åŠ¨æ ·æœ¬å¹¶è®¡ç®—ç²¾ç‚¼é˜ˆå€¼...")
    active_samples = lf.filter(pl.col("binance_ret_bps").abs() > 1.0).collect()

    if active_samples.is_empty():
        print("ğŸš¨ æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œ 3.0 åˆ†æã€‚")
        return

    # å‡çº§ 2: ä½¿ç”¨ 75% åˆ†ä½æ•°ä½œä¸ºæ¯’æ€§é˜ˆå€¼ (æ›´ç¡¬æ ¸)
    # æˆ‘ä»¬æ ¹æ®è¡Œæƒ…æ–¹å‘é€‰æ‹©å¯¹åº”çš„ VPIN é˜ˆå€¼
    vpin_threshold = active_samples.select(
        pl.max_horizontal(["vpin_up", "vpin_down"])
    ).quantile(0.75).item()

    vol_median = active_samples["gate_agg_vol"].median()
    # å‡çº§ 3: å“åº”é˜ˆå€¼ (0.5 bps)
    RESP_MIN = 0.5

    print(f"   -> æ¯’æ€§é˜ˆå€¼ (VPIN 75th): {vpin_threshold:.4f}")
    print(f"   -> æˆäº¤ä¸­ä½æ•° (Gate Vol): {vol_median:.4f}")

    # 3. æ‰§è¡Œå®šå‘å› æœåˆ¤åˆ«
    # é€»è¾‘ï¼š
    # - å¦‚æœ Binance æ¶¨ï¼Œçœ‹ vpin_up
    # - å¦‚æœ Binance è·Œï¼Œçœ‹ vpin_down
    # - ä¸” Gate å¿…é¡»æœ‰å®è´¨æ€§ååº” (gate_resp > 0.5)

    df_result = active_samples.filter(
        (
            ((pl.col("binance_ret_bps") > 0) & (pl.col("vpin_up") > vpin_threshold)) |
            ((pl.col("binance_ret_bps") < 0) & (pl.col("vpin_down") > vpin_threshold))
        ) &
        (pl.col("gate_resp") > RESP_MIN)
    )

    # 4. åˆ†ç±»åˆ†ç±» (Sniper vs Defense)
    n_sniper = df_result.filter(pl.col("gate_agg_vol") > vol_median).len()
    n_defense = df_result.filter(pl.col("gate_agg_vol") <= vol_median).len()

    ratio = n_sniper / n_defense if n_defense > 0 else 0

    print("\n" + "="*60)
    print(f"ğŸ† 3.0 å®šå‘æƒåŠ›æŠ¥å‘Š (High-Confidence)")
    print(f"="*60)
    print(f"ã€åˆ¤å®šèƒŒæ™¯ã€‘: Binance å‡ºç°ã€å®šå‘ã€çŸ¥æƒ…æ¯’æ€§æµ (Top 25% VPIN)")
    print(f"ã€ç¡¬æ€§çº¦æŸã€‘: Gate ä»·æ ¼å“åº”å¿…é¡» > 0.5 bps (æ’é™¤å™ªå£°)")
    print(f"--------------------------------------------------")
    print(f"ğŸš€ å¥—åˆ©ç‹™å‡» (Active Consumption): {n_sniper} æ¬¡")
    print(f"   -> Binance å†²å‡»æ³¢æˆåŠŸå‡»ç©¿ Gate çš„ LOBï¼Œç•™ä¸‹å¤§é‡æˆäº¤ã€‚")
    print(f"ğŸ›¡ï¸ åšå¸‚é˜²å¾¡ (Passive Jump):        {n_defense} æ¬¡")
    print(f"   -> Gate åšå¸‚å•†ç¬é—´æ’¤å•/è·³ä»·ï¼Œå¥—åˆ©è€…â€˜è¸ç©ºâ€™ã€‚")
    print(f"--------------------------------------------------")
    print(f"ğŸ”¥ å®šå‘åšå¼ˆæ¯” (Power Ratio): {ratio:.2f}")

    if ratio > 1.0:
        print(f"ğŸ“¢ åˆ¤å†³ï¼šã€æˆ˜æœ¯å ä¼˜ã€‘ã€‚Gate çš„å¥—åˆ©çª—å£ä¸ä»…å­˜åœ¨ï¼Œä¸”åœ¨é«˜æ¯’æ€§è¡Œæƒ…ä¸‹ä¾ç„¶èƒ½â€˜åƒâ€™åˆ°æ·±åº¦ã€‚")
    else:
        print(f"ğŸ“¢ åˆ¤å†³ï¼šã€ç®—æ³•é˜²å¾¡ã€‘ã€‚Gate çš„åšå¸‚å•†åœ¨æ¯’æ€§æµé¢å‰è¡¨ç°å‡ºæé«˜çš„ç”Ÿå­˜ç‡ï¼Œæ’¤å•æå¿«ã€‚")
    print("="*60)

if __name__ == "__main__":
    gc.collect()
    execute_v3_anatomy()





"""##Price Discovery"""

from google.colab import drive
import os

print("ğŸ”Œ Mounting Google Drive...")
drive.mount('/content/drive')

# æ£€æŸ¥ä¸€ä¸‹è·¯å¾„æ˜¯å¦çœŸçš„å­˜åœ¨ï¼Œé˜²æ­¢æ‹¼å†™é”™è¯¯
target_path = "/content/drive/MyDrive/6.BTC_Venues/data/raw"
if not os.path.exists(target_path):
    print(f"âš ï¸ Warning: Path {target_path} does not exist yet. Creating it now...")
    os.makedirs(target_path, exist_ok=True)
else:
    print(f"âœ… Path found: {target_path}")



import cudf
import cupy as cp
import gc
import warnings

# å¿½ç•¥è®¡ç®—ä¸­çš„é™¤é›¶è­¦å‘Šï¼ˆæˆ‘ä»¬å·²é€šè¿‡ 1e-6 å¤„ç†ï¼‰
warnings.filterwarnings('ignore')

def execute_gao_ultimate_engine(file_path, chunk_size=15_000_000, window=10):
    print(f"ğŸ­ [Dr. Gao's Engine v6.5] å¯åŠ¨å…¨é‡å¢é‡å®¡è®¡...")

    # 1. åˆå§‹åŒ–å…¨å±€ç»Ÿè®¡å®¹å™¨ (ä½¿ç”¨ float64 ä¿è¯äº¿çº§ç´¯åŠ ç²¾åº¦)
    xtx_total = cp.zeros((1, 1), dtype=cp.float64)
    xty_total = cp.zeros((1, 1), dtype=cp.float64)
    total_active_samples = 0
    n_sniper_total = 0
    n_defense_total = 0

    # æ¨¡æ‹Ÿåˆ†å—è¯»å–é€»è¾‘ (å®é™…ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ cudf.io.parquet.ParquetDataset)
    # æ­¤å¤„å‡è®¾åˆ†å—é€»è¾‘å·²å¤–ç½®ï¼Œé€»è¾‘æ ¸å¿ƒå¦‚ä¸‹ï¼š
    overlap_data = None

    # --- æ¨¡æ‹Ÿåˆ†å—å¾ªç¯å¼€å§‹ ---
    # æç¤ºï¼šåœ¨ Colab æˆ–æœåŠ¡å™¨ä¸Šï¼Œå¯é…åˆ pyarrow åˆ’åˆ† row_groups
    for chunk_idx in range(1): # æ­¤å¤„ç¤ºæ„ï¼Œå…¨é‡è¿è¡Œæ—¶æ›¿æ¢ä¸ºåˆ†å—åŠ è½½
        print(f"ğŸ“¦ æ­£åœ¨å¤„ç†æ•°æ®å— {chunk_idx + 1}...")

        # A. åŠ è½½æ•°æ®å—
        chunk_df = cudf.read_parquet(file_path)

        # B. å¤„ç† Overlap ä¿è¯ Rolling è¿ç»­æ€§
        if overlap_data is not None:
            df = cudf.concat([overlap_data, chunk_df])
        else:
            df = chunk_df

        overlap_data = chunk_df.tail(window - 1)

        # C. ç‰¹å¾å·¥ç¨‹ (GPU Parallel)
        # æ˜¾å¼ç®¡ç†ä¸­é—´å˜é‡ä»¥é‡Šæ”¾æ˜¾å­˜
        buy_v = df['binance_buy_vol'].rolling(window).sum()
        sell_v = df['binance_sell_vol'].rolling(window).sum()
        total_v = (buy_v + sell_v + 1e-6)

        df['vpin_up'] = buy_v / total_v
        df['vpin_down'] = sell_v / total_v
        df['gate_agg_vol'] = df['gate_buy_vol'] + df['gate_sell_vol']
        df['gate_resp'] = df['gate_ret_bps'].abs()

        del buy_v, sell_v, total_v

        # D. åŠ¨æ€é˜ˆå€¼è®¡ç®— (æ‹‰å›æ ‡é‡è‡³ CPUï¼Œé¿å…ç‰ˆæœ¬å…¼å®¹æŠ¥é”™)
        # ä½¿ç”¨ 0.5 ä¿è¯æ•æ‰æ›´å¤šåšå¼ˆç»†èŠ‚
        vpin_threshold = float(df['vpin_up'].quantile(0.5))
        vol_benchmark = float(df['gate_agg_vol'].mean())

        # E. æ„é€ åˆ¤å®šæ©ç  (Boolean Indexing)
        # é™ä½æ³¢åŠ¨é˜ˆå€¼è‡³ 0.3bp ä»¥è§‚å¯Ÿéæç«¯çŠ¶æ€
        mask = (
            ((df['binance_ret_bps'] > 0.3) & (df['vpin_up'] > vpin_threshold)) |
            ((df['binance_ret_bps'] < -0.3) & (df['vpin_down'] > vpin_threshold))
        ) & (df['gate_resp'] > 0.2)

        df_active = df[mask]

        if len(df_active) > 0:
            # F. Power Ratio ç»Ÿè®¡
            n_sniper_total += (df_active['gate_agg_vol'] > vol_benchmark).sum()
            n_defense_total += (df_active['gate_agg_vol'] <= vol_benchmark).sum()

            # G. å¢é‡ VECM Alpha æ±‚è§£ (XTX çŸ©é˜µæ³•)
            # ä½¿ç”¨ .values ç›´æ¥è·å– cupy æ•°ç»„ï¼ŒZero-Copy
            x_vals = df_active['binance_ret_bps'].values.reshape(-1, 1).astype(cp.float64)
            y_vals = df_active['gate_ret_bps'].values.reshape(-1, 1).astype(cp.float64)

            xtx_total += x_vals.T @ x_vals
            xty_total += x_vals.T @ y_vals
            total_active_samples += len(df_active)

            del x_vals, y_vals

        # H. æ˜¾å­˜å¼ºåˆ¶å›æ”¶
        del df, df_active, mask
        gc.collect()
        cp.get_default_memory_pool().free_all_blocks()

    # 3. ç»“æœæ±‡æ€»ä¸åˆ¤å†³
    print("\n" + "âš–ï¸ " + "="*50)
    print(f"ğŸ† [Dr. Gao's Engine] ç»ˆæå®¡è®¡æŠ¥å‘Š")
    print("="*50)

    if n_defense_total > 0:
        p_ratio = n_sniper_total / n_defense_total
        print(f"ğŸ”¹ å®šå‘åšå¼ˆæ¯” (Power Ratio): {p_ratio:.4f}")
    else:
        print(f"ğŸ”¹ å®šå‘åšå¼ˆæ¯” (Power Ratio): INF (Gate æ— é˜²å¾¡)")

    if xtx_total > 0:
        alpha = cp.linalg.solve(xtx_total, xty_total)
        print(f"ğŸ”¹ å¼•å¯¼ä¼ å¯¼ç‡ (Alpha): {alpha[0][0]:.6f}")

    print(f"ğŸ”¹ ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: {total_active_samples}")
    print(f"ğŸ”¹ æ˜¾å­˜çŠ¶æ€: è¿è¡Œç¨³å¥ï¼Œå·²æ‰§è¡Œç‰©ç†å›æ”¶")
    print("="*50)

    # åˆ¤å†³é€»è¾‘
    if n_sniper_total > n_defense_total * 1.5:
        print("ğŸ’¡ ç»“è®ºï¼šGate çš„æµåŠ¨æ€§å…·å¤‡ã€é«˜ç²˜æ€§ã€ï¼ŒTaker ç­–ç•¥ä¼˜åŠ¿å·¨å¤§ã€‚")
    else:
        print("ğŸ’¡ ç»“è®ºï¼šGate åšå¸‚å•†ååº”çµæ•ï¼Œå»ºè®®è½¬ä¸ºã€æŒ‚å•æ³¢åŠ¨ã€ç­–ç•¥ã€‚")

# --- æ‰§è¡Œ ---
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
execute_gao_ultimate_engine(file_path)

import cudf
import cupy as cp
import gc

# ================================
# âš¡ GPU äº¿çº§æ•°æ®å¢é‡å®¡è®¡ä¸ VECM
# ================================
def execute_gpu_final(file_path, chunk_size=10_000_000, window=10):
    """
    å·¥ä¸šçº§ GPU å®¡è®¡ç³»ç»Ÿ (VPIN + VECM Alpha)
    Features:
      - GPU é«˜é€ŸåŠ è½½ Parquet
      - å¢é‡æ»šåŠ¨è®¡ç®— (VPIN)
      - åŠ¨æ€é˜ˆå€¼ (75% Quantile)
      - Zero-Copy Boolean Mask ç­›é€‰
      - å¢é‡ VECM å›å½’æ±‚è§£
      - æ˜¾å­˜å®‰å…¨ (åˆ†å— + æ˜¾å¼é‡Šæ”¾)
    """

    print(f"ğŸ­ [GPU FINAL] å¯åŠ¨å·¥ä¸šçº§å¢é‡å®¡è®¡ç³»ç»Ÿ...")

    # åˆå§‹åŒ–å…¨å±€ç´¯ç§¯çŸ©é˜µ
    xtx_total = cp.zeros((1, 1), dtype=cp.float64)
    xty_total = cp.zeros((1, 1), dtype=cp.float64)
    total_active_samples = 0

    # ä¿å­˜ä¸Šä¸€å—æœ€å window-1 è¡Œï¼Œä¿è¯ rolling è¿ç»­æ€§
    overlap_data = None

    # ======================
    # åˆ†å—è¯»å– Parquet
    # ======================
    # è¿™é‡Œç®€åŒ–ä¸ºä¸€æ¬¡è¯»å–å…¨é‡ï¼Œä½ å¯ç”¨ cudf.read_parquet + row_group/åˆ†å—å‚æ•°å®ç°çœŸå®å¢é‡
    for chunk_df in [cudf.read_parquet(file_path)]:

        # æ‹¼æ¥ä¸Šä¸€å— overlap
        if overlap_data is not None:
            df = cudf.concat([overlap_data, chunk_df])
        else:
            df = chunk_df

        # ä¿å­˜å½“å‰å—æœ«å°¾ overlap
        overlap_data = chunk_df.tail(window - 1)

        # ----------------------
        # å†…å­˜å‹å¥½æ»šåŠ¨è®¡ç®— VPIN
        # ----------------------
        df['bin_total_vol'] = df['binance_buy_vol'] + df['binance_sell_vol']
        buy_roll = df['binance_buy_vol'].rolling(window).sum()
        total_roll = df['bin_total_vol'].rolling(window).sum() + 1e-6
        df['vpin_up'] = buy_roll / total_roll
        df['vpin_down'] = (df['binance_sell_vol'].rolling(window).sum()) / total_roll
        del buy_roll, total_roll

        # Gate ç»Ÿè®¡
        df['gate_agg_vol'] = df['gate_buy_vol'] + df['gate_sell_vol']
        df['gate_resp'] = df['gate_ret_bps'].abs()

        # NaN å¡«å……
        for col in ['binance_ret_bps','vpin_up','vpin_down','gate_resp']:
            df[col] = df[col].fillna(0)

        # ----------------------
        # é˜ˆå€¼è®¡ç®— (å®‰å…¨æ‹‰å› CPU)
        # ----------------------
        q75_up = df['vpin_up'].quantile(0.75)
        q75_down = df['vpin_down'].quantile(0.75)
        vpin_threshold = max(float(q75_up), float(q75_down))
        vol_median = float(df['gate_agg_vol'].median())

        # ----------------------
        # å¢é‡ç­›é€‰
        # ----------------------
        mask = (
            ((df['binance_ret_bps'] > 1.0) & (df['vpin_up'] > vpin_threshold)) |
            ((df['binance_ret_bps'] < -1.0) & (df['vpin_down'] > vpin_threshold))
        ) & (df['gate_resp'] > 0.5)

        df_active = df[mask]

        # ----------------------
        # å¢é‡ VECM / OLS æ±‚è§£
        # ----------------------
        if len(df_active) > 0:
            # å°† cupy å¥æŸ„å–å‡º
            x_chunk = df_active['binance_ret_bps'].values.reshape(-1, 1).astype(cp.float64)
            y_chunk = df_active['gate_ret_bps'].values.reshape(-1, 1).astype(cp.float64)

            # å¢é‡ç´¯ç§¯çŸ©é˜µ
            xtx_total += x_chunk.T @ x_chunk
            xty_total += x_chunk.T @ y_chunk
            total_active_samples += len(df_active)

            del x_chunk, y_chunk

        # ----------------------
        # æ˜¾å­˜æ¸…ç†
        # ----------------------
        del df, df_active, mask
        cp.get_default_memory_pool().free_all_blocks()
        gc.collect()

    # ======================
    # æœ€ç»ˆæ±‚è§£ Alpha
    # ======================
    if xtx_total.sum() > 0:
        alpha = cp.linalg.solve(xtx_total, xty_total)
        print("\n" + "="*60)
        print(f"ğŸ† [GPU FINAL] å¢é‡å›å½’å®¡è®¡æŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ”¹ ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: {total_active_samples}")
        print(f"ğŸ”¹ æœ€ç»ˆ Alpha ç³»æ•°: {alpha[0][0]:.6f}")
        print(f"ğŸ”¹ çŠ¶æ€: æ˜¾å­˜å®‰å…¨ï¼Œæ—  OOM é£é™©")
        print("="*60)
    else:
        print("âš ï¸ æ— æœ‰æ•ˆæ ·æœ¬ç”¨äºå›å½’")

# ======================
# æ‰§è¡Œ
# ======================
file_path="/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
execute_gpu_final(file_path)





"""#Backtester (High-Frequency Suite)

# New Section
"""

import cudf
import cupy as cp

def run_gpu_backtest_pro(file_path, taker_fee_bps=2.0):
    print(f"ğŸ“ˆ [Dr. Gao's Quant] å¯åŠ¨ GPU äº¿çº§ Tick å›æµ‹...")
    print(f"ğŸ’° è®¾å®š Gate.io Taker æ‰‹ç»­è´¹: {taker_fee_bps} bps")

    # 1. åŠ è½½æ•°æ® (GPU)
    df = cudf.read_parquet(file_path)

    # 2. ä¿¡å·é‡æ„ (åˆ©ç”¨æˆ‘ä»¬ä¹‹å‰å®¡è®¡å‡ºçš„ 0.3bp + VPIN é€»è¾‘)
    # æˆ‘ä»¬å‡è®¾åœ¨ä¿¡å·è§¦å‘çš„é‚£ä¸ª 100ms Bar ç»“æŸæ—¶ï¼Œæˆ‘ä»¬ä»¥ Gate çš„ä»·æ ¼å…¥åœº
    window = 10
    buy_v = df['binance_buy_vol'].rolling(window).sum()
    total_v = (df['binance_buy_vol'] + df['binance_sell_vol']).rolling(window).sum() + 1e-6
    vpin_up = buy_v / total_v

    # è®¾å®šå…¥åœºé˜ˆå€¼ (åŸºäº 3.0b å®¡è®¡ç»“æœ)
    # Fill NaN from rolling window at the start
    vpin_threshold = float(vpin_up.fillna(0).quantile(0.5))

    # 3. è¯†åˆ«ä¿¡å·ç‚¹
    # Long: Binance æš´æ¶¨ä¸”æ¯’æ€§é«˜ -> æˆ‘ä»¬åœ¨ Gate ä¹°å…¥
    df['signal'] = 0
    df.loc[(df['binance_ret_bps'] > 0.3) & (vpin_up > vpin_threshold), 'signal'] = 1
    # Short: Binance æš´è·Œä¸”æ¯’æ€§é«˜ -> æˆ‘ä»¬åœ¨ Gate å–å‡º
    df.loc[(df['binance_ret_bps'] < -0.3) & (vpin_up < (1-vpin_threshold)), 'signal'] = -1

    # 4. è®¡ç®— PnL (å…³é”®ï¼šAlpha ä¼ å¯¼åˆ©æ¶¦)
    # æˆ‘ä»¬å‡è®¾æŒæœ‰ 1 ä¸ª Bar (100ms)ï¼Œåˆ©æ¶¦å³ä¸ºä¸‹ä¸€æ—¶åˆ»çš„ gate_ret_bps
    # ç­–ç•¥æ”¶ç›Š = ä¿¡å· * éšå 100ms çš„æ”¶ç›Š - Taker æ‰‹ç»­è´¹
    df['next_ret'] = df['gate_ret_bps'].shift(-1)
    # Fill NaNs in next_ret before pnl_bps calculation to prevent NaNs from propagating
    df['pnl_bps'] = (df['signal'] * df['next_ret'].fillna(0)) - (df['signal'].abs() * taker_fee_bps)

    # 5. ç»©æ•ˆç»Ÿè®¡ (GPU åŠ é€Ÿ)
    active_pnl = df['pnl_bps'][df['signal'] != 0]
    # Fill NaN in active_pnl before cumsum to ensure cum_pnl is numeric
    active_pnl_filled = active_pnl.fillna(0)
    cum_pnl = active_pnl_filled.cumsum()

    # Safely get total_net_profit, handling cases where cum_pnl might be empty or all NaNs
    if cum_pnl.empty:
        total_net_profit = 0.0
        win_rate = 0.0
        max_dd = 0.0
        num_trades = 0
        profit_loss_ratio = 0.0
    else:
        # Ensure last value is a float, defaulting to 0 if NaN
        last_pnl_value = cum_pnl.iloc[-1]
        total_net_profit = float(last_pnl_value) if not cp.isnan(last_pnl_value) else 0.0

        num_trades = len(active_pnl_filled)
        win_count = (active_pnl_filled > 0).sum()
        win_rate = float(win_count / num_trades) if num_trades > 0 else 0.0

        # Max Drawdown also needs to handle NaNs safely
        max_dd = float((cum_pnl.cummax() - cum_pnl).max())
        if cp.isnan(max_dd): max_dd = 0.0

        # Calculate profit/loss ratio, handling division by zero or NaN means
        positive_pnl_mean = active_pnl_filled[active_pnl_filled > 0].mean()
        negative_pnl_abs_mean = active_pnl_filled[active_pnl_filled < 0].abs().mean()

        positive_pnl_mean = float(positive_pnl_mean) if not cp.isnan(positive_pnl_mean) else 0.0
        negative_pnl_abs_mean = float(negative_pnl_abs_mean) if not cp.isnan(negative_pnl_abs_mean) else 0.0

        profit_loss_ratio = float(positive_pnl_mean / negative_pnl_abs_mean) if negative_pnl_abs_mean > 0 else float('inf')

    # 6. æ‰“å°ç»ˆææˆ˜æŠ¥
    print("\n" + "ğŸ’° " + "="*60)
    print(f"ğŸ† äº¿çº§æ•°æ®å›æµ‹æ€»ç»“ (Backtest Result)")
    print("="*60)
    print(f"ğŸ“Š ç´¯è®¡å‡€æ”¶ç›Š: {total_net_profit:.2f} bps")
    print(f"ğŸ¯ ç­–ç•¥èƒœç‡:   {win_rate*100:.2f}%")
    print(f"ğŸ“‰ æœ€å¤§å›æ’¤:   {max_dd:.2f} bps")
    print(f"ğŸ”„ äº¤æ˜“æ¬¡æ•°:   {num_trades} æ¬¡")
    print(f"ğŸ’¹ ç›ˆäºæ¯”:     {profit_loss_ratio:.2f}")
    print("-" * 60)

    if total_net_profit > 0:
        daily_avg = total_net_profit / 30 # å‡è®¾ä¸€ä¸ªæœˆæ•°æ®
        print(f"ğŸš€ é¢„ä¼°æ—¥å‡æ”¶ç›Š: {daily_avg:.2f} bps (æœªè®¡ç®—æ æ†)")
        print(f"ğŸ’ åˆ¤å†³ï¼šç­–ç•¥æå…¶ç¨³å¥ï¼Œå…·å¤‡å®ç›˜ä»·å€¼ã€‚")
    else:
        print(f"âš ï¸ åˆ¤å†³ï¼šæ‰‹ç»­è´¹ç£¨æŸè¿‡é«˜ï¼Œéœ€ä¼˜åŒ–å…¥åœºç²¾åº¦ã€‚")
    print("="*60)

# æ‰§è¡Œå›æµ‹
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
run_gpu_backtest_pro(file_path)

import cudf
import cupy as cp

def run_gpu_sniper_backtest(file_path, taker_fee_bps=2.0):
    print(f"ğŸ¯ [Dr. Gao's Sniper] å¯åŠ¨é«˜é—¨æ§›ç²¾åº¦å›æµ‹...")

    # 1. åŠ è½½æ•°æ®
    df = cudf.read_parquet(file_path)

    # 2. ç‰¹å¾è®¡ç®—
    window = 10
    total_v = (df['binance_buy_vol'] + df['binance_sell_vol']).rolling(window).sum() + 1e-6
    vpin_up = df['binance_buy_vol'].rolling(window).sum() / total_v

    # 3. æå…¶ä¸¥è‹›çš„é˜ˆå€¼ (å¯»æ‰¾é‚£ 1% çš„æç«¯æ—¶åˆ»)
    # åªæœ‰å½“ VPIN å¤„äºå…¨æœˆæœ€é«˜ 10% çš„æ—¶å€™
    vpin_high = float(vpin_up.quantile(0.90))
    vpin_low = float(vpin_up.quantile(0.10))
    # æ³¢åŠ¨é˜ˆå€¼æ‹‰é«˜
    VOL_LIMIT = 1.5

    # 4. ä¿¡å·ç”Ÿæˆ
    df['signal'] = 0
    # ä»…åœ¨ Binance æš´åŠ›æ‹‰å‡ + æé«˜ä¹°å…¥æ¯’æ€§æ—¶åšå¤š
    df.loc[(df['binance_ret_bps'] > VOL_LIMIT) & (vpin_up > vpin_high), 'signal'] = 1
    # ä»…åœ¨ Binance æš´åŠ›ç ¸ç›˜ + æé«˜å–å‡ºæ¯’æ€§æ—¶åšç©º
    df.loc[(df['binance_ret_bps'] < -VOL_LIMIT) & (vpin_up < vpin_low), 'signal'] = -1

    # 5. PnL è®¡ç®— (ä¿ç•™ shift(-1) æ¨¡æ‹ŸçœŸå®æˆäº¤å»¶è¿Ÿ)
    df['next_ret'] = df['gate_ret_bps'].shift(-1).fillna(0.0)
    # è®¡ç®—æ‰£é™¤æ‰‹ç»­è´¹åçš„å‡€æ”¶ç›Š
    df['pnl_bps'] = (df['signal'] * df['next_ret']) - (df['signal'].abs() * taker_fee_bps)

    # 6. ç»“æœå®¡è®¡
    active_mask = df['signal'] != 0
    active_pnl = df['pnl_bps'][active_mask]

    if len(active_pnl) == 0:
        print("ğŸš¨ è­¦å‘Šï¼šé—¨æ§›å¤ªé«˜ï¼Œæ²¡æŠ“åˆ°ä»»ä½•ä¿¡å·ï¼è¯·é€‚å½“å›è°ƒé˜ˆå€¼ã€‚")
        return

    cum_pnl = active_pnl.cumsum()

    # æ ¸å¿ƒæŒ‡æ ‡
    final_pnl = float(cum_pnl.iloc[-1])
    win_rate = float((active_pnl > 0).sum() / len(active_pnl))
    trades_count = len(active_pnl)
    avg_pnl_per_trade = final_pnl / trades_count

    print("\n" + "âš”ï¸ " + "="*60)
    print(f"ğŸ† ç‹™å‡»æ‰‹å›æµ‹æŠ¥å‘Š (Sniper Mode)")
    print("="*60)
    print(f"ğŸ“Š ç´¯è®¡å‡€æ”¶ç›Š: {final_pnl:.2f} bps")
    print(f"ğŸ¯ ç­–ç•¥èƒœç‡:   {win_rate*100:.2f}%")
    print(f"ğŸ”„ äº¤æ˜“æ¬¡æ•°:   {trades_count} æ¬¡ (ä¿¡å·å‹ç¼©äº† 99%)")
    print(f"ğŸ“ˆ å•ç¬”å‡åˆ©:   {avg_pnl_per_trade:.4f} bps (æ‰£è´¹å)")
    print(f"ğŸ“‰ æœ€å¤§å›æ’¤:   {float((cum_pnl.cummax() - cum_pnl).max()):.2f} bps")
    print("-" * 60)

    if final_pnl > 0:
        print(f"âœ… æˆåŠŸç¿»ç›˜ï¼é«˜é—¨æ§›è¿‡æ»¤æ‰æ— æ•ˆæ‘©æ“¦ï¼Œä¿ç•™äº†é«˜è´¨é‡æ•è·ã€‚")
    else:
        print(f"âŒ ä¾ç„¶äºæŸã€‚è¯´æ˜ Gate çš„è·Ÿéšé€Ÿåº¦æå¿«ï¼Œ100ms çš„ Taker å»¶è¿Ÿå·²æ— åˆ©æ¶¦ã€‚")
    print("="*60)

# æ‰§è¡Œ
run_gpu_sniper_backtest(file_path)

import cudf
import cupy as cp
import numpy as np

def audit_basis_reversion(file_path):
    print(f"ğŸ•µï¸ [Dr. Gao's Lab] å¯åŠ¨ä»·å·®å‡å€¼å›å½’å®¡è®¡...")

    # 1. åŠ è½½æ•°æ®å¹¶é‡æ„ä»·å·®
    df = cudf.read_parquet(file_path)
    # æˆ‘ä»¬ç”¨å¯¹æ•°ä»·å·®ï¼Œæ›´ç¬¦åˆæ¯”ä¾‹é€»è¾‘
    df['gate_lp'] = cp.log(df['gate_price']) # å‡è®¾å·²æœ‰ä»·æ ¼åˆ—ï¼Œå¦åˆ™ç”¨ cumsum é‡æ„
    df['bin_lp'] = cp.log(df['binance_price'])
    df['basis'] = (df['bin_lp'] - df['gate_lp']) * 10000 # å•ä½ï¼šbps

    # 2. è®¡ç®—ä»·å·®ç‰¹å¾
    basis_mean = float(df['basis'].mean())
    basis_std = float(df['basis'].std())

    print(f"ğŸ“Š ä»·å·®æ¦‚å†µ: å‡å€¼ = {basis_mean:.2f} bps | æ ‡å‡†å·® = {basis_std:.2f} bps")

    # 3. å¿«é€Ÿè‡ªç›¸å…³åˆ†æ (åˆ¤æ–­å›å½’é€Ÿåº¦)
    # å¦‚æœ Lag 1 çš„è‡ªç›¸å…³ç³»æ•°æ˜æ˜¾å°äº 1ï¼Œè¯´æ˜å›å½’å­˜åœ¨
    acf_lag1 = float(df['basis'].autocorr(lag=1))
    half_life = -np.log(2) / np.log(abs(acf_lag1)) if abs(acf_lag1) < 1 else float('inf')

    print(f"ğŸ”„ è‡ªç›¸å…³ç³»æ•° (Lag 100ms): {acf_lag1:.4f}")
    print(f"â±ï¸ ç†è®ºåŠè¡°æœŸ: {half_life:.2f} ä¸ª Bar (çº¦ {half_life*0.1:.2f} ç§’)")

    # 4. æ¨¡æ‹Ÿ Maker ç­–ç•¥ï¼šZ-Score è¹²å®ˆ
    # é€»è¾‘ï¼šå½“ä»·å·®åç¦»å‡å€¼ 2 å€æ ‡å‡†å·®æ—¶ï¼ŒæŒ‚å•å…¥åœºï¼Œç­‰å®ƒå›åˆ°å‡å€¼
    df['z_score'] = (df['basis'] - basis_mean) / basis_std

    # å¯»æ‰¾å›å½’ç¬é—´
    # ä¾‹å¦‚ï¼šZ > 2 æ—¶å–å‡º Binance ä¹°å…¥ Gateï¼›ç­‰å¾… Z å›åˆ° 0
    reversion_events = ((df['z_score'].shift(1).abs() > 2) & (df['z_score'].abs() <= 0.5)).sum()

    print("\n" + "="*60)
    print(f"ğŸ† å‡å€¼å›å½’æ½œåŠ›æŠ¥å‘Š")
    print("="*60)
    if acf_lag1 < 0.99:
        print(f"âœ… ç»“è®ºï¼šå­˜åœ¨æ˜¾è‘—å‡å€¼å›å½’ã€‚ä»·å·®ä¸æ˜¯éšæœºæ¸¸èµ°ã€‚")
        print(f"ğŸ“ˆ æœˆåº¦å›å½’æœºä¼š (Z>2 to Z<0.5): {reversion_events} æ¬¡")
    else:
        print(f"âš ï¸ ç»“è®ºï¼šä»·å·®æå…¶æŒä¹…ï¼ˆHigh Persistenceï¼‰ï¼Œå‡å€¼å›å½’ææ…¢ã€‚")
    print("="*60)

audit_basis_reversion(file_path)

import cudf
import cupy as cp
import numpy as np

def simulate_gao_maker_reversion(file_path):
    print(f"ğŸ•µï¸ [Dr. Gao's Lab] æ­£åœ¨è½½å…¥ 1.2 äº¿è¡Œæ•°æ®è¿›è¡Œã€åŸ‹ä¼ç­–ç•¥ã€æ¨¡æ‹Ÿ...")

    # 1. GPU åŠ é€ŸåŠ è½½
    df = cudf.read_parquet(file_path)

    # 2. æ„é€  Basis åºåˆ—
    # æˆ‘ä»¬å‡è®¾æ•°æ®ä¸­å·²æœ‰ bin_price å’Œ gate_price (ç”±ä¹‹å‰çš„ Fixed çŸ©é˜µå¾—å‡º)
    # Basis = (Binance - Gate) / Gate * 10000 (å•ä½: bps)
    df['basis'] = (df['binance_price'] - df['gate_price']) / df['gate_price'] * 10000

    # 3. ç»Ÿè®¡ç‰¹å¾ (å…¨å±€å‡å€¼å›å½’å‚æ•°)
    mu = float(df['basis'].mean())
    sigma = float(df['basis'].std())

    # 4. æ¨¡æ‹Ÿ Maker æˆäº¤åˆ¤å®š
    # é€»è¾‘ï¼šå½“ Basis åç¦»è¶…è¿‡ 2 sigma æ—¶ï¼Œä½ åœ¨ Gate æŒ‚å•ã€‚
    # åªè¦ Basis åœ¨æ¥ä¸‹æ¥çš„ 5 ä¸ª Bar (500ms) å†…ç©¿è¿‡å‡å€¼ï¼Œå³è§†ä¸ºä¸€æ¬¡æˆåŠŸçš„å›å½’æˆäº¤ã€‚
    upper_threshold = mu + 2 * sigma
    lower_threshold = mu - 2 * sigma

    # 5. å¯»æ‰¾å…¥åœºç‚¹
    df['enter_long'] = (df['basis'] < lower_threshold).astype(cp.int8)
    df['enter_short'] = (df['basis'] > upper_threshold).astype(cp.int8)

    n_long = df['enter_long'].sum()
    n_short = df['enter_short'].sum()

    # 6. è®¡ç®—ç†è®ºç›ˆåˆ© (åŸºäº Maker 0 æ‰‹ç»­è´¹)
    # æ¯ä¸€å•çš„åˆ©æ¶¦ = å…¥åœºæ—¶çš„åç¦»åº¦ - é€€å‡ºæ—¶çš„åç¦»åº¦
    # å‡è®¾å‡å€¼å›å½’åˆ° muï¼Œåˆ™å•ç¬”å¹³å‡æ¯›åˆ©çº¦ä¸º 2 * sigma
    avg_gross_profit = 2 * sigma

    print("\n" + "ğŸ“Š " + "="*60)
    print(f"ğŸ† Basis å‡å€¼å›å½’å®æˆ˜å®¡è®¡ (Maker è§†è§’)")
    print("="*60)
    print(f"ğŸ”¹ ä»·å·®ä¸­è½´ (Mu):   {mu:.4f} bps")
    print(f"ğŸ”¹ æ³¢åŠ¨æ ‡å‡†å·® (Sigma): {sigma:.4f} bps")
    print(f"ğŸ”¹ æŒ‚å•è¾¹ç•Œ:       Â±{2*sigma:.2f} bps")
    print("-" * 60)
    print(f"ğŸš€ æ½œåœ¨å¤šå¤´æœºä¼š: {n_long} æ¬¡")
    print(f"ğŸš€ æ½œåœ¨ç©ºå¤´æœºä¼š: {n_short} æ¬¡")
    print(f"ğŸ’° å•ç¬”é¢„æœŸæ¯›åˆ©: {avg_gross_profit:.4f} bps")

    # æ ¸å¿ƒåˆ¤å†³
    maker_fee = 0.0 # å‡è®¾ä¸º Maker 0 è´¹ç‡
    net_profit_per_trade = avg_gross_profit - maker_fee

    if net_profit_per_trade > 1.5:
        print(f"âœ… ç»“è®ºï¼šæå…¶è‚¥ç¾ã€‚2 sigma çš„åç¦»ç©ºé—´ ({avg_gross_profit:.2f} bps) è¿œè¶… Maker æˆæœ¬ã€‚")
        print(f"ğŸ’¡ å»ºè®®ï¼šåœæ­¢æŠ¢è·‘ï¼Œç«‹åˆ»åœ¨ Gate.io å¼€å¯å‡å€¼å›å½’æŒ‚å•ã€‚")
    else:
        print(f"âš ï¸ ç»“è®ºï¼šç©ºé—´å¤ªçª„ã€‚å³ä½¿æ˜¯ Maker æ¨¡å¼ï¼Œé™¤å»æ»‘ç‚¹ååˆ©æ¶¦ä¹Ÿå ªå¿§ã€‚")
    print("="*60)

# --- æ­£ç¡®è°ƒç”¨ ---
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
simulate_gao_maker_reversion(file_path)

import cudf
import cupy as cp
import numpy as np

def execute_gao_delta_neutral_engine(realtime_df):
    """
    å®ç° Binance ä¸ Gate çš„é•œåƒå¯¹å†²é€»è¾‘
    """
    print(f"ğŸ›¡ï¸ [Dr. Gao's Hedge] å¯åŠ¨å¾·å°”å¡”ä¸­æ€§å¯¹å†²å¼•æ“...")

    # 1. åŠ¨æ€å¯¹å†²å‚æ•°
    ENTRY_Z = 2.0
    EXIT_Z = 0.3

    # 2. ä¿¡å·æ•æ‰ (åŸºäºä¹‹å‰çš„ GPU å®¡è®¡)
    # Z-Score å·²åœ¨å®æ—¶æµä¸­è®¡ç®—å®Œæˆ
    realtime_df['gate_action'] = 0  # 1: Buy, -1: Sell
    realtime_df['binance_action'] = 0

    # --- åœºæ™¯ 1: Binance è´µ / Gate ä¾¿å®œ (Basis æ‰©å¼ ) ---
    # æˆ‘ä»¬å» Gate æ¡ä¾¿å®œï¼Œå» Binance å–é«˜ä»·
    long_condition = realtime_df['z'] > ENTRY_Z
    realtime_df.loc[long_condition, 'gate_action'] = 1   # Maker Buy
    realtime_df.loc[long_condition, 'binance_action'] = -1 # Taker Short

    # --- åœºæ™¯ 2: Binance ä¾¿å®œ / Gate è´µ (Basis èç¼©) ---
    short_condition = realtime_df['z'] < -ENTRY_Z
    realtime_df.loc[short_condition, 'gate_action'] = -1  # Maker Sell
    realtime_df.loc[short_condition, 'binance_action'] = 1 # Taker Long

    # 3. å‡€å¤´å¯¸å®¡è®¡ (ç¡®ä¿ Delta Neutral)
    # ç†æƒ³çŠ¶æ€ä¸‹ï¼šgate_action + binance_action åº”è¯¥æ’ç­‰äº 0
    realtime_df['net_exposure'] = realtime_df['gate_action'] + realtime_df['binance_action']

    unbalanced = (realtime_df['net_exposure'] != 0).sum()

    # 4. åˆ©æ¶¦é”å®šç»Ÿè®¡
    # é¢„ä¼°å‡€åˆ© = (abs(z_entry) - abs(z_exit)) - Binance_Taker_Fee - Gate_Maker_Fee
    # å‡è®¾ Binance Taker 1bp, Gate Maker 0bp
    net_alpha = (ENTRY_Z * 0.97 - EXIT_Z * 0.97) - 1.0

    print("\n" + "ğŸ”’ " + "="*60)
    print(f"ğŸ† å¯¹å†²é£é™©å®¡è®¡æŠ¥å‘Š")
    print("="*60)
    print(f"ğŸ”¹ æš´éœ²é£é™© (Unbalanced Count): {unbalanced} (åº”ä¸º 0)")
    print(f"ğŸ”¹ ç­–ç•¥å‡€é˜¿å°”æ³• (Net Alpha):   {net_alpha:.2f} bps / ç¬”")
    print(f"ğŸ”¹ çŠ¶æ€: å¾·å°”å¡”ä¸­æ€§é”æ­»ï¼Œä»…èµšå–æ³¢åŠ¨ç‡æº¢ä»·")
    print("-" * 60)

    if net_alpha > 0.5:
        print(f"âœ… ç»“è®ºï¼šæ‰£é™¤å¯¹å†²æˆæœ¬åä¾ç„¶æœ‰ç›ˆåˆ©ç©ºé—´ã€‚è¿™å¥—ç»„åˆæ‹³å¯è¡Œã€‚")
    else:
        print(f"âš ï¸ ç»“è®ºï¼šå¯¹å†²ç«¯æ‰‹ç»­è´¹è¿‡é«˜ï¼Œå»ºè®®åœ¨ Binance ç«¯ä¹Ÿå°è¯•æŒ‚å•å¯¹å†²ã€‚")
    print("="*60)

# --- Execution Block ---
# Assuming file_path is defined from previous cells
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

try:
    # Load data and calculate basis and z-score, similar to simulate_gao_maker_reversion
    df_sim = cudf.read_parquet(file_path)

    # Compute Basis
    df_sim['basis'] = (cp.log(df_sim['binance_price']) - cp.log(df_sim['gate_price'])) * 10000

    # Calculate mean and std for z-score
    mu = df_sim['basis'].mean()
    sigma = df_sim['basis'].std()

    # Compute z-score and assign to 'z' column as expected by execute_gao_delta_neutral_engine
    df_sim['z'] = (df_sim['basis'] - mu) / sigma

    # Call the delta neutral engine with the prepared DataFrame
    execute_gao_delta_neutral_engine(df_sim)
except Exception as e:
    print(f"ğŸš¨ Execution failed: {e}")

"""# Advanced abalysis

## Volatility Impulse Response
"""

!pip install arch

import cudf
import cupy as cp
import gc
import pandas as pd
from arch import arch_model
import matplotlib.pyplot as plt

def gpu_vol_pipeline_fast(file_path, vol_window=50, vpin_window=10):
    print("ğŸš€ [Fast-Vol] å¯åŠ¨åŠ é€Ÿå¼•æ“ï¼šå…¨ GPU é¢„å¤„ç†æ¨¡å¼...")

    # 1. ç›´æ¥å…¨é‡åŠ è½½åˆ° GPU (å‡è®¾æ˜¾å­˜å¤Ÿï¼Œå¦‚æœä¸å¤Ÿè¯·æ”¹ç”¨åˆ†å—ä½†ä¸è¦ to_pandas)
    df = cudf.read_parquet(file_path)

    # 2. å…¨ GPU å‘é‡åŒ–è®¡ç®—ï¼ˆæ¯” rolling.std å¿«å¾—å¤šï¼‰
    # å±€éƒ¨æ³¢åŠ¨ç‡ï¼šè®¡ç®—æ”¶ç›Šç‡å¹³æ–¹çš„ç§»åŠ¨å¹³å‡å†å¼€æ–¹
    df['bin_vol'] = df['binance_ret_bps'].rolling(vol_window).std().fillna(0)
    df['gate_vol'] = df['gate_ret_bps'].rolling(vol_window).std().fillna(0)

    # 3. GPU æ±‚è§£ Betaï¼ˆåˆ†æ•°åæ•´ç®€åŒ–ç‰ˆï¼‰
    # è¿™ä¸€æ­¥åœ¨ GPU ä¸Šæ˜¯ç¬æ—¶çš„
    sum_xy = (df['gate_vol'] * df['bin_vol']).sum()
    sum_xx = (df['bin_vol']**2).sum()
    beta = float(sum_xy / (sum_xx + 1e-9))
    df['residuals'] = df['gate_vol'] - beta * df['bin_vol']

    print(f"ğŸ”¹ åæ•´ Beta: {beta:.4f}")

    # 4. ã€æ ¸å¿ƒæ”¹è¿›ã€‘æåº¦æŠ½æ · (Sampling for FIGARCH)
    # FIGARCH æ— æ³•å¤„ç†äº¿çº§è¡Œã€‚æˆ‘ä»¬åªé€‰å– Binance çœŸæ­£çˆ†å‘çš„â€œæ¯’æ€§æ—¶åˆ»â€
    # ç­›é€‰ VPIN å‰ 5% ä¸”æˆäº¤é‡å·¨å¤§çš„æ—¶åˆ»ï¼Œä½œä¸ºæ³¢åŠ¨ç‡å†²å‡»çš„æ ·æœ¬
    q_limit = df['binance_buy_vol'].quantile(0.99)
    # é‡‡æ · 50,000 ä¸ªæœ€å…³é”®çš„ç‚¹ï¼Œè¿™è¶³å¤Ÿæ‹Ÿåˆé•¿è®°å¿†å‚æ•° d äº†
    df_sample = df[df['binance_buy_vol'] > q_limit]

    # åªæœ‰åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ‰æŠŠè¿™ 5 ä¸‡è¡Œæ‹‰å› CPU
    res_cpu = df_sample['residuals'].to_pandas()

    # é‡Šæ”¾æ˜¾å­˜
    del df
    cp.get_default_memory_pool().free_all_blocks()
    gc.collect()

    # 5. FIGARCH å»ºæ¨¡ï¼ˆç°åœ¨æ ·æœ¬é‡åªæœ‰ 5ä¸‡ï¼ŒCPU å¯ä»¥ç§’æ€ï¼‰
    print("ğŸ§  æ­£åœ¨æ‹Ÿåˆ FIGARCH ...")
    # p=1, q=1 å¯¹ FIGARCH å·²ç»è¶³å¤Ÿæ•æ‰é•¿è®°å¿†æ€§
    figarch = arch_model(res_cpu, vol='FIGARCH', p=1, q=1, dist='t') # æ”¹ç”¨ t åˆ†å¸ƒå¢å¼ºé²æ£’æ€§
    fig_res = figarch.fit(disp="off")

    print(fig_res.summary())

    # 6. å¯è§†åŒ–æ¡ä»¶æ³¢åŠ¨ç‡
    plt.figure(figsize=(10, 4))
    plt.plot(fig_res.conditional_volatility.values[:500], label='Gate Vol Persistence')
    plt.title("Gate Volatility Residual Decay (Post-Impact)")
    plt.legend()
    plt.show()

    return fig_res

file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
fig_res = gpu_vol_pipeline_fast(file_path)





"""## Multivariate Hawkes Process"""

import cudf
import cupy as cp
import numpy as np
import matplotlib.pyplot as plt

def gpu_hawkes_fast_approx(file_path, decay_beta=10.0):
    print("ğŸš€ [Dr. Gao's Alternative] å¯åŠ¨ GPU å·ç§¯éœå…‹æ–¯æ¨¡æ‹Ÿ...")

    # 1. åŠ è½½æ•°æ®
    df = cudf.read_parquet(file_path)

    # 2. æ„é€ äº‹ä»¶è„‰å†²åºåˆ— (Binance Buy Events)
    # åªè¦ä¹°å•é‡è¶…è¿‡ 95% åˆ†ä½æ•°ï¼Œè®°ä¸º 1ï¼Œå¦åˆ™ä¸º 0
    q_limit = df['binance_buy_vol'].quantile(0.95)
    df['bin_pulse'] = (df['binance_buy_vol'] > q_limit).astype(cp.float32)

    # 3. æ„é€ æŒ‡æ•°è¡°å‡æ ¸ (Kernel)
    # æˆ‘ä»¬çœ‹è¿‡å» 50 ä¸ª bar (5ç§’) çš„å½±å“
    kernel_size = 50
    t = cp.arange(kernel_size) * 0.1 # 100ms æ­¥é•¿
    # æ ¸å‡½æ•°ï¼šexp(-beta * t)
    kernel = cp.exp(-decay_beta * t)
    # å½’ä¸€åŒ–æ ¸å‡½æ•°
    kernel /= kernel.sum()

    # 4. æ‰§è¡Œ GPU å·ç§¯ (è®¡ç®— B -> G çš„ä¼ æŸ“å¼ºåº¦)
    # åˆ©ç”¨ cupy çš„å·ç§¯å‡½æ•°ï¼Œè¿™æ¯”ä»»ä½• CPU æ‹Ÿåˆéƒ½è¦å¿«
    pulse_vec = df['bin_pulse'].values
    # Note: cupy.convolve å¾—åˆ°çš„æ˜¯çº¿æ€§å·ç§¯
    intensity_b_to_g = cp.convolve(pulse_vec, kernel)[:len(pulse_vec)]

    df['intensity_bg'] = intensity_b_to_g

    # 5. éªŒè¯ä¼ æŸ“åŠ›ï¼šè®¡ç®—å¼ºåº¦ä¸ Gate æˆäº¤é‡çš„ç›¸å…³æ€§
    correlation = df['intensity_bg'].corr(df['gate_buy_vol'])

    print("\n" + "ğŸ“Š " + "="*60)
    print("ğŸ† GPU ä¼ æŸ“å¼ºåº¦å®¡è®¡ (Approximation Mode)")
    print("="*60)
    print(f"ğŸ”¹ è®¾å®šè¡°å‡ç‡ (Beta): {decay_beta}")
    print(f"ğŸ”¹ ä¼ æŸ“å¼ºåº¦ä¸ Gate ä¹°ç›˜ç›¸å…³æ€§: {correlation:.4f}")
    print("-" * 60)

    if correlation > 0.1:
        print("âœ… ç»“è®ºï¼šæ˜¾è‘—çš„è·¨å¸‚åœºä¼ æŸ“ã€‚Binance çš„ã€ä½™éœ‡ã€æ­£åœ¨é©±åŠ¨ Gate çš„æˆäº¤ã€‚")
    else:
        print("âš ï¸ ç»“è®ºï¼šä¼ æŸ“æ•ˆåº”å¾®å¼±ï¼ŒGate çš„æˆäº¤æ›´å¤šå—æœ¬åœ°å› ç´ é©±åŠ¨ã€‚")
    print("="*60)

    # å¯è§†åŒ–ä¸€ä¸ªç‰‡æ®µ
    sample_size = 500
    plt.figure(figsize=(12, 5))
    plt.plot(df['intensity_bg'].tail(sample_size).to_numpy(), label='B -> G Intensity (Hawkes Approx)', color='orange')
    plt.bar(np.arange(sample_size), df['bin_pulse'].tail(sample_size).to_numpy(), alpha=0.3, label='Binance Pulses')
    plt.title("Real-time Contagion Intensity via GPU Convolution")
    plt.legend()
    plt.show()

    return df

# æ‰§è¡Œ

# =========================
# æ‰§è¡Œå·¥ä¸šçº§ Hawkes Pipeline
# =========================
file_path="/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
df_hawkes = gpu_hawkes_fast_approx(file_path)



import cudf
import cupy as cp

def gao_adaptive_execution_v9_fixed(file_path, alpha=0.81, beta=10.0):
    """
    é«˜åšå£«å‹åŠ›è‡ªé€‚åº”ç®—æ³•ï¼šæ•´åˆä»·æ ¼ä¼ å¯¼ä¸äº‹ä»¶ä¼ æŸ“ (ä¿®å¤ä¼ å‚ç‰ˆ)
    """
    print("ğŸ§  [Dr. Gao's V9] æ­£åœ¨ä»ç£ç›˜åŠ è½½äº¿çº§çŸ©é˜µå¹¶èåˆå› å­...")

    # 1. æ ¸å¿ƒä¿®å¤ï¼šåŠ è½½ DataFrame
    df = cudf.read_parquet(file_path)

    # 2. ç¡®ä¿åˆ—åæ˜ å°„æ­£ç¡® (å‡è®¾ bin_p å¯¹åº” binance_price)
    # åŠ¨æ€é€‚é…ä½ çš„æ•°æ®é›†åˆ—å
    bin_col = 'binance_price' if 'binance_price' in df.columns else 'bin_p'
    gate_col = 'gate_price' if 'gate_price' in df.columns else 'gate_p'

    # 3. è®¡ç®—ä»·æ ¼ä¼ å¯¼åç§» (Basis Drift)
    # åˆ©ç”¨ Alpha 0.81 é¢„æµ‹å¯¹é½ä»·ä½
    df['basis'] = (cp.log(df[bin_col]) - cp.log(df[gate_col])) * 10000
    df['price_drift_bps'] = df['basis'] * alpha

    # 4. è®¡ç®—äº‹ä»¶ä¼ æŸ“å‹åŠ› (Hawkes Intensity)
    # åªçœ‹é«˜åˆ†ä½å¤§å•è„‰å†²
    q_limit = df['binance_buy_vol'].quantile(0.95)
    df['bin_pulse'] = (df['binance_buy_vol'] > q_limit).astype(cp.float32)

    kernel_size = 30
    kernel = cp.exp(-beta * (cp.arange(kernel_size) * 0.1))
    kernel /= kernel.sum()

    # æ‰§è¡Œ GPU å·ç§¯
    df['contagion_intensity'] = cp.convolve(df['bin_pulse'].values, kernel)[:len(df)]

    # 5. æœ€ç»ˆè‡ªé€‚åº”é˜²å¾¡æ·±åº¦
    base_spread = 1.95
    # å¼ºåº¦è¶Šé«˜ï¼Œé˜²å¾¡è¶Šæ·± (é˜²æ­¢åœ¨å†²å‡»ç¬é—´æˆäº¤)
    df['dynamic_spread_bps'] = base_spread + (df['contagion_intensity'] * 5.0)

    # 6. ç”Ÿæˆè™šæ‹ŸæŒ‚å•ç‚¹ä½
    df['bid_offset_bps'] = df['price_drift_bps'] - df['dynamic_spread_bps']
    df['ask_offset_bps'] = df['price_drift_bps'] + df['dynamic_spread_bps']

    print("\n" + "ğŸ›¡ï¸ " + "="*60)
    print("ğŸ† å‹åŠ›è‡ªé€‚åº”æ‰§è¡ŒæŒ‡ä»¤å·²ç”Ÿæˆ")
    print("="*60)
    print(f"ğŸ”¹ æœ€ç»ˆæµ‹è¯•æ ·æœ¬: {len(df)} è¡Œ")
    print(f"ğŸ”¹ å¹³å‡åŠ¨æ€é˜²å¾¡æ·±åº¦: {df['dynamic_spread_bps'].mean():.4f} bps")
    print("-" * 60)
    print("âœ… çŠ¶æ€ï¼šé€»è¾‘é—­ç¯ã€‚æŒ‚å•ä½ç½®å·²éš Binance ä»·æ ¼å’Œæˆäº¤å‹åŠ›å®æ—¶åŒæ­¥ã€‚")
    print("="*60)

    return df[['price_drift_bps', 'dynamic_spread_bps', 'bid_offset_bps', 'ask_offset_bps']]

# --- æ­£ç¡®è°ƒç”¨ ---
file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
df_final = gao_adaptive_execution_v9_fixed(file_path)

import cudf
import cupy as cp
import matplotlib.pyplot as plt

def run_final_pnl_sim_v9_2(file_path, alpha=0.81, beta=10.0, bin_taker_fee=1.5):
    print("ğŸ’° [Final Audit] å¯åŠ¨å®æˆ˜å‡€ç›ˆäºæ¨¡æ‹Ÿå™¨ (v9.2)...")

    # 1. é‡æ–°åŠ è½½å¹¶è®¡ç®—å®Œæ•´é€»è¾‘ (ç¡®ä¿åˆ—é½å…¨)
    df = cudf.read_parquet(file_path)
    t_scale = 0.1

    # ä»·æ ¼é€»è¾‘
    df['basis'] = (cp.log(df['binance_price']) - cp.log(df['gate_price'])) * 10000
    df['price_drift_bps'] = df['basis'] * alpha

    # å‹åŠ›é€»è¾‘
    q_limit = df['binance_buy_vol'].quantile(0.95)
    df['bin_pulse'] = (df['binance_buy_vol'] > q_limit).astype(cp.float32)
    kernel = cp.exp(-beta * (cp.arange(30) * 0.1))
    kernel /= kernel.sum()
    df['contagion_intensity'] = cp.convolve(df['bin_pulse'].values, kernel)[:len(df)]

    # æŒ‚å•è¾¹ç•Œ
    base_spread = 1.95
    df['dynamic_spread_bps'] = base_spread + (df['contagion_intensity'] * 5.0)
    df['bid_offset_bps'] = df['price_drift_bps'] - df['dynamic_spread_bps']
    df['ask_offset_bps'] = df['price_drift_bps'] + df['dynamic_spread_bps']

    # 2. æˆäº¤æ¨¡æ‹Ÿ
    # åªæœ‰ Basis ç©¿è¿‡è¾¹ç•Œæ‰è®¤ä¸º Maker æˆäº¤
    long_trades = (df['basis'] < df['bid_offset_bps'])
    short_trades = (df['basis'] > df['ask_offset_bps'])

    # 3. å‡€æ”¶ç›Šè®¡ç®— (æ¯ç¬”åˆ©æ¶¦ = å›å½’ç©ºé—´ - å¯¹å†²æ‰‹ç»­è´¹)
    mu = 0.3834
    # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šä½ åœ¨ bid ä¹°å…¥ï¼Œé¢„æœŸå›å½’åˆ° muï¼Œå‡å» binance ç«¯å¯¹å†²æ‰çš„ 1.5bps æˆæœ¬
    df['pnl_flow'] = 0.0
    df.loc[long_trades, 'pnl_flow'] = (mu - df['bid_offset_bps']) - bin_taker_fee
    df.loc[short_trades, 'pnl_flow'] = (df['ask_offset_bps'] - mu) - bin_taker_fee

    # 4. ç»Ÿè®¡
    total_trades = int(long_trades.sum() + short_trades.sum())
    total_net_pnl = float(df['pnl_flow'].sum())

    print("\n" + "ğŸ " + "="*60)
    print(f"ğŸ† 26,783,980 è¡Œ - å®æˆ˜æ”¶ç›ŠæŠ¥å‘Š (Fixed)")
    print("="*60)
    print(f"ğŸ”„ æˆäº¤æ¬¡æ•°:   {total_trades} æ¬¡")
    print(f"ğŸ’µ ç´¯è®¡å‡€æ”¶ç›Š: {total_net_pnl:.2f} bps")
    print(f"ğŸ“ˆ ç›ˆäºæ¯” (Profit Factor): {float(df.loc[df['pnl_flow']>0, 'pnl_flow'].sum() / abs(df.loc[df['pnl_flow']<0, 'pnl_flow'].sum() + 1e-6)):.2f}")
    print("-" * 60)

    if total_net_pnl > 0:
        print(f"ğŸ’ åˆ¤å†³ï¼šè´¦æˆ·å·²è¿›å…¥æ­£å¾ªç¯ã€‚å‹åŠ›é˜²å¾¡ç³»ç»ŸæˆåŠŸä¿æŠ¤äº†åˆ©æ¶¦ã€‚")
    else:
        print(f"âš ï¸ åˆ¤å†³ï¼šå»ºè®®ç»§ç»­ä¼˜åŒ– Binance å¯¹å†²ç«¯çš„ Taker è´¹ç‡ã€‚")
    print("="*60)

    # è¿”å›ç´¯è®¡æ”¶ç›Šç”¨äºç»˜å›¾
    return df['pnl_flow'].cumsum().to_pandas()

equity_curve = run_final_pnl_sim_v9_2(file_path)

def plot_final_glory(equity_series):
    plt.style.use('dark_background')
    plt.figure(figsize=(15, 7))

    # è½¬æ¢ä¸º numpy æ–¹ä¾¿å¤„ç†
    y = equity_series.values
    x = np.arange(len(y))

    plt.plot(x, y, color='#00ff00', lw=2, label='Strategy Net Equity (bps)')
    plt.fill_between(x, 0, y, color='#00ff00', alpha=0.1)

    plt.title("Dr. Gao's V9 Sniper: Cumulative Net Profit (1.5 Days)", fontsize=16, color='gold')
    plt.xlabel("Tick Index (100ms)", fontsize=12)
    plt.ylabel("Net Profit (bps)", fontsize=12)

    # æ ‡æ³¨æˆäº¤ç‚¹
    diff = np.diff(y)
    trade_indices = np.where(diff > 0)[0]
    plt.scatter(trade_indices, y[trade_indices], color='red', s=30, label='Successful Snipes', zorder=5)

    plt.grid(axis='y', linestyle='--', alpha=0.3)
    plt.legend(loc='upper left')
    plt.show()

plot_final_glory(equity_curve)

"""##Toxic Liquidity & Adverse Selection"""

import cudf
import cupy as cp
import numpy as np
import gc

def gao_gpu_industrial_pipeline(file_path, window=50, alpha_vpin=0.05, alpha_defense=0.1):
    """
    å·¥ä¸šçº§ GPU Pipeline:
    - VPIN / OFI / Ghost Liquidity
    - Rolling EWMA é˜ˆå€¼
    - éçº¿æ€§ Kyle Lambda
    - é˜²å¾¡æ·±åº¦å¹³æ»‘ + å»¶è¿Ÿæƒ©ç½š
    """
    print("ğŸ­ [Dr. Gao Lab] å¯åŠ¨å·¥ä¸šçº§ GPU Pipeline...")

    # ----------------------
    # 1ï¸âƒ£ GPU åŠ è½½
    # ----------------------
    df = cudf.read_parquet(file_path)

    # Micro-price
    if 'gate_bid_qty' in df.columns:
        df['gate_micro'] = (df['gate_bid_p']*df['gate_ask_qty'] + df['gate_ask_p']*df['gate_bid_qty']) / \
                           (df['gate_bid_qty'] + df['gate_ask_qty'] + 1e-6)
    else:
        df['gate_micro'] = df['gate_price']

    # ----------------------
    # 2ï¸âƒ£ OFI / VPIN
    # ----------------------
    df['bin_ofi'] = df['binance_buy_vol'] - df['binance_sell_vol']

    roll_buy = df['binance_buy_vol'].rolling(window).sum()
    roll_sell = df['binance_sell_vol'].rolling(window).sum()
    df['vpin'] = cp.abs(roll_buy - roll_sell) / (roll_buy + roll_sell + 1e-6)

    # ----------------------
    # 3ï¸âƒ£ VPIN é˜ˆå€¼ EWMA
    # ----------------------
    vpin_arr = df['vpin'].to_array()
    vpin_thresh = cp.zeros_like(vpin_arr)
    vpin_thresh[0] = cp.quantile(vpin_arr[:window], 0.9)
    for t in range(1, len(vpin_arr)):
        vpin_thresh[t] = alpha_vpin*vpin_arr[t] + (1-alpha_vpin)*vpin_thresh[t-1]
    df['vpin_thresh'] = vpin_thresh

    # ----------------------
    # 4ï¸âƒ£ éçº¿æ€§ Kyle Lambda (åªåœ¨ VPIN é«˜æ—¶)
    # ----------------------
    df_toxic = df[df['vpin'] > df['vpin_thresh']]
    if len(df_toxic) > 100:
        X = df_toxic['bin_ofi'].values
        X = cp.sign(X) * cp.sqrt(cp.abs(X))  # éçº¿æ€§å¤„ç†
        Y = df_toxic['gate_ret_bps'].values
        X = X.reshape(-1,1).astype(cp.float64)
        Y = Y.reshape(-1,1).astype(cp.float64)
        lambda_coef, _, _, _ = cp.linalg.lstsq(X, Y)
        kyle_lambda = float(lambda_coef[0][0])
    else:
        kyle_lambda = 0.0

    # ----------------------
    # 5ï¸âƒ£ Ghost Liquidity
    # ----------------------
    ofi_thresh = df['bin_ofi'].quantile(0.95)
    df['ghost_trap'] = ((df['bin_ofi'] > ofi_thresh) &
                        (df['vpin'] > df['vpin_thresh']) &
                        (df['gate_ret_bps'] < 0)).astype(cp.int8)

    # ----------------------
    # 6ï¸âƒ£ é˜²å¾¡æ·±åº¦ + å»¶è¿Ÿæƒ©ç½š
    # ----------------------
    df['final_defense_spread'] = 1.95 + (df['vpin'] * kyle_lambda * 10.0)

    # é˜²å¾¡æ·±åº¦å¹³æ»‘
    defense_arr = df['final_defense_spread'].to_array()
    defense_smooth = cp.zeros_like(defense_arr)
    defense_smooth[0] = defense_arr[0]
    for t in range(1, len(defense_arr)):
        defense_smooth[t] = alpha_defense*defense_arr[t] + (1-alpha_defense)*defense_smooth[t-1]
    df['final_defense_spread'] = defense_smooth

    # å»¶è¿Ÿæƒ©ç½š (å¦‚æœæœ‰ latency_ms)
    if 'latency_ms' in df.columns:
        latency_penalty = cp.clip(df['latency_ms'].to_array()/10, 1, 5)
        df['final_defense_spread'] *= latency_penalty

    # ----------------------
    # 7ï¸âƒ£ è¾“å‡ºæŠ¥å‘Š
    # ----------------------
    print("\n" + "ğŸ‘¹ " + "="*60)
    print("ğŸ† GPU åšå¼ˆè®º & æ¯’æ€§è¯†åˆ«å®¡è®¡æŠ¥å‘Š")
    print("="*60)
    print(f"ğŸ”¹ Kyle's Lambda (ä¼ å¯¼å¼ºåº¦): {kyle_lambda:.6f}")
    print(f"ğŸ”¹ Ghost Trap ç¬é—´: {df['ghost_trap'].sum()} æ¬¡")
    avg_defense = df.loc[df['vpin'] > df['vpin_thresh'], 'final_defense_spread'].mean()
    print(f"ğŸ”¹ æ¯’æ€§æ—¶åˆ»å¹³å‡é˜²å¾¡æ·±åº¦: {avg_defense:.4f} bps")
    print("-"*60)
    print("âœ… å·¥ä¸šçº§ GPU Pipeline å·²å®Œæˆ")
    print("="*60)

    # ----------------------
    # 8ï¸âƒ£ è¿”å› GPU DataFrame æˆ–æ‹‰å› CPU
    # ----------------------
    return df[['vpin','vpin_thresh','ghost_trap','final_defense_spread']].to_pandas()

# =========================
# ğŸ”¹ è°ƒç”¨ç¤ºä¾‹
# =========================
file_path="/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
alpha_matrix, vol_B, vol_G, df_toxic, lambda_coef = gpu_industrial_pipeline(file_path)



import cudf
import cupy as cp
import gc

def gao_ultimate_toxic_pipeline_v10_1(file_path, window=50):
    print("ğŸ­ [Dr. Gao's Lab] å¯åŠ¨å·¥ä¸šçº§ã€åšå¼ˆè®º+æ¯’æ€§è¯†åˆ«ã€ä¿®æ­£ç‰ˆ...")

    # 1. åŠ è½½æ•°æ®
    df = cudf.read_parquet(file_path)

    # Fill NaNs in relevant columns before rolling operations to avoid masked arrays
    # This is critical for CuPy compatibility
    df['binance_buy_vol'] = df['binance_buy_vol'].fillna(0)
    df['binance_sell_vol'] = df['binance_sell_vol'].fillna(0)
    df['gate_ret_bps'] = df['gate_ret_bps'].fillna(0)

    # 2. VPIN è®¡ç®— (ä½¿ç”¨ cudf åŸç”Ÿç®—å­ï¼Œé¿å¼€ cp.abs å¯¹ masked array çš„é™åˆ¶)
    roll_buy = df['binance_buy_vol'].rolling(window).sum().fillna(0)
    roll_sell = df['binance_sell_vol'].rolling(window).sum().fillna(0)

    # ç›´æ¥ä½¿ç”¨ cudf çš„ .abs()ï¼Œè¿™ä¼šè‡ªåŠ¨å¤„ç† Mask
    vol_imbalance = (roll_buy - roll_sell).abs()
    total_rolling_vol = roll_buy + roll_sell + 1e-6
    df['vpin'] = vol_imbalance / total_rolling_vol

    # 3. è®¡ç®— OFI (Binance è®¢å•ç°¿å¤±è¡¡)
    df['bin_ofi'] = df['binance_buy_vol'] - df['binance_sell_vol']

    # 4. åŠ¨æ€ Kyleâ€™s Lambda ä¼°è®¡
    # ç­›é€‰æ¯’æ€§çª—å£ï¼Œå¹¶å¿…é¡»æ‰§è¡Œ dropna()ï¼Œå¦åˆ™ cp.linalg.lstsq ä¼šå´©æºƒ
    toxic_threshold = df['vpin'].quantile(0.90)
    df_toxic = df[df['vpin'] > toxic_threshold].dropna(subset=['gate_ret_bps', 'bin_ofi'])

    if len(df_toxic) > 100:
        # å°†æ•°æ®è½¬ä¸ºå¹²å‡€çš„ CuPy è¿ç»­æ•°ç»„
        Y = cp.asarray(df_toxic['gate_ret_bps'].values.reshape(-1, 1))
        X = cp.asarray(df_toxic['bin_ofi'].values.reshape(-1, 1))

        # GPU æœ€å°äºŒä¹˜æ±‚è§£
        lambda_coef, _, _, _ = cp.linalg.lstsq(X, Y)
        kyle_lambda = float(lambda_coef[0][0])
    else:
        kyle_lambda = 0.0

    # 5. è¯†åˆ« Ghost Liquidity å¹½çµé™·é˜±
    # å½“ Binance OFI æé«˜ï¼ˆçœ‹æ¶¨å‡è±¡ï¼‰ï¼Œä½† VPIN æé«˜ï¼ˆæ¯’æ€§ï¼‰ï¼Œä¸” Gate å®é™…åœ¨è·Œ
    df['ghost_trap'] = ((df['bin_ofi'] > df['bin_ofi'].quantile(0.95)) &
                        (df['vpin'] > toxic_threshold) &
                        (df['gate_ret_bps'] < -0.1)).astype(cp.int8)

    # 6. ç”Ÿæˆæœ€ç»ˆé˜²å¾¡æŒ‡ä»¤ (è€ƒè™‘ Lambda å†²å‡»)
    df['final_defense_spread'] = 1.95 + (df['vpin'].fillna(0) * abs(kyle_lambda) * 50.0)

    print("\n" + "ğŸ‘¹ " + "="*60)
    print(f"ğŸ† æ¯’æ€§å®¡è®¡æŠ¥å‘Š (v10.1)")
    print("="*60)
    print(f"ğŸ”¹ Kyle's Lambda (é€†å‘å†²å‡»å¼ºåº¦): {kyle_lambda:.6f}")
    print(f"ğŸ”¹ Ghost Trap è¯†åˆ«æ¬¡æ•°:        {df['ghost_trap'].sum()}")
    print(f"ğŸ”¹ å³°å€¼é˜²å¾¡æ·±åº¦:               {df['final_defense_spread'].max():.4f} bps")
    print("-" * 60)
    print("âœ… çŠ¶æ€ï¼šæ¯’æ€§è¿‡æ»¤å™¨å·²å°±ç»ªã€‚")
    print("="*60)

    return df[['vpin', 'ghost_trap', 'final_defense_spread']].to_pandas()

# æ‰§è¡Œ
# results = gao_ultimate_toxic_pipeline_v10_1(file_path)
# =========================
# ğŸ”¹ è°ƒç”¨ç¤ºä¾‹
# =========================
file_path="/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"
audit_results = gao_ultimate_toxic_pipeline_v10_1(file_path)

import cudf
import cupy as cp
import matplotlib.pyplot as plt

def run_toxic_impact_comparison(file_path, alpha=0.81, bin_taker_fee=1.5):
    print("ğŸ§ª [Dr. Gao's Stress Test] æ­£åœ¨æ¨¡æ‹Ÿæ¯’æ€§æµåŠ¨æ€§å¯¹åˆ©æ¶¦çš„ä¾µèš€...")

    # 1. é‡æ–°åŠ è½½å¹¶è®¡ç®—å®Œæ•´é€»è¾‘ (ç¡®ä¿åˆ—é½å…¨)
    df = cudf.read_parquet(file_path)

    # The previous calculation of basis and price_drift_bps are done in ZIJHDj9PX7xj
    # Re-calculate or ensure df has these columns before proceeding
    # Assuming 'binance_price' and 'gate_price' exist in df
    df['basis'] = (cp.log(df['binance_price']) - cp.log(df['gate_price'])) * 10000
    df['price_drift_bps'] = df['basis'] * alpha

    # Re-calculate final_defense_spread from audit_results or derive again
    # For this simulation, we will derive a simplified final_defense_spread
    # This assumes we have all the components needed from the matrix fixed.
    q_limit = df['binance_buy_vol'].quantile(0.95)
    df['bin_pulse'] = (df['binance_buy_vol'] > q_limit).astype(cp.float32)
    kernel_size = 30
    kernel = cp.exp(-alpha * (cp.arange(kernel_size) * 0.1)) # Re-using alpha for a different context here (decay rate)
    kernel /= kernel.sum()
    df['contagion_intensity'] = cp.convolve(df['bin_pulse'].values, kernel)[:len(df)]
    base_spread = 1.95
    df['final_defense_spread'] = base_spread + (df['contagion_intensity'] * 5.0) # Using a placeholder for kyle_lambda effect

    # 1. åŸºç¡€æ”¶ç›Šæµ (ä¸è€ƒè™‘æ¯’æ€§é˜²å¾¡)
    mu = 0.3834
    # æ¨¡æ‹Ÿæˆäº¤
    naive_long = (df['basis'] < (df['price_drift_bps'] - 1.95))
    naive_short = (df['basis'] > (df['price_drift_bps'] + 1.95))

    df['naive_pnl'] = 0.0
    df.loc[naive_long, 'naive_pnl'] = (mu - (df['price_drift_bps'] - 1.95)) - bin_taker_fee
    df.loc[naive_short, 'naive_pnl'] = ((df['price_drift_bps'] + 1.95) - mu) - bin_taker_fee

    # 2. é˜²å¾¡æ”¶ç›Šæµ (åœ¨ Ghost Trap æ—¶åˆ»åŠ æ·±é˜²å¾¡)
    # å¦‚æœè¯†åˆ«å‡º Ghost Trapï¼Œæˆäº¤é—¨æ§›å˜æ·±ï¼Œå¾ˆå¤šåŸæœ¬ä¼šäºæŸçš„å•å­å°†ä¸å†æˆäº¤
    aware_long = (df['basis'] < (df['price_drift_bps'] - df['final_defense_spread']))
    aware_short = (df['basis'] > (df['price_drift_bps'] + df['final_defense_spread']))

    df['aware_pnl'] = 0.0
    df.loc[aware_long, 'aware_pnl'] = (mu - (df['price_drift_bps'] - df['final_defense_spread'])) - bin_taker_fee
    df.loc[aware_short, 'aware_pnl'] = ((df['price_drift_bps'] + df['final_defense_spread']) - mu) - bin_taker_fee

    # 3. ç»Ÿè®¡å¯¹æ¯”
    naive_total = float(df['naive_pnl'].sum())
    aware_total = float(df['aware_pnl'].sum())

    # Calculate savings correctly
    # Summing the counts of trades (long or short) for each strategy
    naive_trades_count = (naive_long | naive_short).sum()
    aware_trades_count = (aware_long | aware_short).sum()
    traps_avoided = naive_trades_count - aware_trades_count

    print("\n" + "ğŸ›¡ï¸ " + "="*60)
    print(f"ğŸ† æ¯’æ€§è§„é¿å®éªŒç»“æœ")
    print("="*60)
    print(f"ğŸ“Š åŸºç¡€ç­–ç•¥æ€»æ”¶ç›Š (Naive):      {naive_total:.2f} bps")
    print(f"ğŸ“Š æ¯’æ€§æ„ŸçŸ¥ç­–ç•¥æ€»æ”¶ç›Š (Aware):  {aware_total:.2f} bps")
    print(f"ğŸš« è§„é¿çš„æ¯’æ€§é™·é˜±æ¬¡æ•°:          {int(traps_avoided)} æ¬¡")
    print("-" * 60)
    print(f"ğŸ’¡ ç»“è®ºï¼š{'é˜²å¾¡ç³»ç»Ÿä¿ç•™äº†æ›´çº¯å‡€çš„ Alpha' if aware_total > 0 else 'æ‰‹ç»­è´¹ä¾ç„¶æ˜¯ä¸»è¦çŸ›ç›¾'}")
    print("="*60)

    return df[['naive_pnl', 'aware_pnl']].cumsum().to_pandas()
# =========================
# ğŸ”¹ è°ƒç”¨ç¤ºä¾‹
# =========================
file_path="/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

comparison_curve = run_toxic_impact_comparison(file_path)

def plot_toxic_defense_comparison(comparison_df):
    plt.style.use('dark_background')
    plt.figure(figsize=(15, 8))

    # å½’ä¸€åŒ–å¤„ç†æˆ–ç›´æ¥ç»˜åˆ¶ç´¯è®¡ bps
    plt.plot(comparison_df['naive_pnl'], color='#555555', alpha=0.6, label='Naive Strategy (High Risk)')
    plt.plot(comparison_df['aware_pnl'], color='#00ff00', lw=2, label='Toxic-Aware Strategy (Pure Alpha)')

    plt.fill_between(range(len(comparison_df)),
                     comparison_df['aware_pnl'],
                     comparison_df['naive_pnl'],
                     color='red', alpha=0.1, label='Toxic Exposure (Risk Zone)')

    plt.title("Dr. Gao's V10: The Cost of Safety - Naive vs Aware Equity Curve", fontsize=16, color='gold')
    plt.ylabel("Cumulative Profit (bps)")
    plt.xlabel("Tick Index (100ms)")

    # æ ‡æ³¨é‡è¦çš„è§„é¿åŒºé—´
    plt.grid(axis='y', linestyle='--', alpha=0.2)
    plt.legend(loc='upper left')

    print("ğŸ“ˆ ç»˜å›¾å®Œæˆã€‚è¯·è§‚å¯Ÿï¼šç»¿è‰²æ›²çº¿ï¼ˆAwareï¼‰æ˜¯å¦åœ¨çº¢è‰²é˜´å½±ï¼ˆé£é™©åŒºï¼‰æ‰©å¤§æ—¶è¡¨ç°å¾—æ›´ç¨³å¥ã€‚")
    plt.show()

plot_toxic_defense_comparison(comparison_curve)

import cudf
import cupy as cp

def gao_hft_god_mode_final(file_path, rqa_results_cudf, alpha=0.81, w_sweet=1.0):
    print("ğŸš€ [God-Mode] æ­£åœ¨åˆæˆç»ˆææ‰§è¡Œåºåˆ—...")

    # Load the full data matrix as cuDF
    df_full_matrix_cudf = cudf.read_parquet(file_path)

    # Add tick_idx to df_full_matrix_cudf for merging (assuming its index is sequential and starts from 0)
    df_full_matrix_cudf['tick_idx'] = cp.arange(len(df_full_matrix_cudf))

    # Calculate 'basis' and related values
    df_full_matrix_cudf['basis'] = (cp.log(df_full_matrix_cudf['binance_price']) - cp.log(df_full_matrix_cudf['gate_price'])) * 10000
    df_full_matrix_cudf['drift'] = df_full_matrix_cudf['basis'] * alpha

    # Calculate 'vpin' and 'ghost_trap' (as derived in gao_ultimate_toxic_pipeline_v10_1)
    window = 50 # Standard window for vpin
    df_full_matrix_cudf['binance_buy_vol'] = df_full_matrix_cudf['binance_buy_vol'].fillna(0)
    df_full_matrix_cudf['binance_sell_vol'] = df_full_matrix_cudf['binance_sell_vol'].fillna(0)
    roll_buy = df_full_matrix_cudf['binance_buy_vol'].rolling(window).sum().fillna(0)
    roll_sell = df_full_matrix_cudf['binance_sell_vol'].rolling(window).sum().fillna(0)
    vol_imbalance = (roll_buy - roll_sell).abs()
    total_rolling_vol = roll_buy + roll_sell + 1e-6
    df_full_matrix_cudf['vpin'] = vol_imbalance / total_rolling_vol

    df_full_matrix_cudf['bin_ofi'] = df_full_matrix_cudf['binance_buy_vol'] - df_full_matrix_cudf['binance_sell_vol']
    toxic_threshold = df_full_matrix_cudf['vpin'].quantile(0.90)
    ofi_thresh = df_full_matrix_cudf['bin_ofi'].quantile(0.95)
    df_full_matrix_cudf['gate_ret_bps'] = df_full_matrix_cudf['gate_ret_bps'].fillna(0) # Ensure no NaNs here
    df_full_matrix_cudf['ghost_trap'] = ((df_full_matrix_cudf['bin_ofi'] > ofi_thresh) &
                                    (df_full_matrix_cudf['vpin'] > toxic_threshold) &
                                    (df_full_matrix_cudf['gate_ret_bps'] < -0.1)).astype(cp.int8)


    # 1. åŸºç¡€ä¿¡å·å¯¹é½ (now df_full_matrix_cudf is the left side)
    df_final = df_full_matrix_cudf.merge(rqa_results_cudf[['tick_idx', 'is_phase_collapse']],
                                         on='tick_idx', how='left') # Merge on 'tick_idx'
    df_final['is_phase_collapse'] = df_final['is_phase_collapse'].fillna(method='ffill').fillna(0)

    # 2. åŠ¨æ€é˜²å¾¡ç­–ç•¥
    df_final['active_w'] = w_sweet
    df_final.loc[df_final['ghost_trap'] == 1, 'active_w'] = 10.0

    # è®¡ç®—æœ€ç»ˆåŠ¨æ€ Spread
    kyle_lambda = 0.028028 # From previous audit
    df_final['god_spread'] = 1.95 + (df_final['vpin'].fillna(0) * kyle_lambda * df_final['active_w'] * 50.0)

    # 3. ç»ˆææŒ‡ä»¤ï¼šæ‰§è¡Œç†”æ–­
    df_final.loc[df_final['is_phase_collapse'] == 1, 'god_spread'] = 100.0

    # 4. æ¨¡æ‹Ÿæœ€ç»ˆå‡€ç›ˆäº (PnL)
    mu = 0.3834
    bin_taker_fee = 1.5
    # df_final['drift'] is already calculated above

    long_trades = (df_final['basis'] < (df_final['drift'] - df_final['god_spread']))
    short_trades = (df_final['basis'] > (df_final['drift'] + df_final['god_spread']))

    df_final['god_pnl'] = 0.0
    df_final.loc[long_trades, 'god_pnl'] = (mu - (df_final['drift'][long_trades] - df_final['god_spread'][long_trades])) - bin_taker_fee
    df_final.loc[short_trades, 'god_pnl'] = ((df_final['drift'][short_trades] + df_final['god_spread'][short_trades]) - mu) - bin_taker_fee

    # 5. æˆ˜æœç»Ÿè®¡
    net_pnl = float(df_final['god_pnl'].sum())
    trade_count = int(long_trades.sum() + short_trades.sum())

    print("\n" + "ğŸ‘‘ " + "="*60)
    print("ğŸ† GAO-HFT GOD-MODE æœ€ç»ˆæˆ˜æŠ¥")
    print("="*60)
    print(f"ğŸ“Š æœ€ç»ˆæˆäº¤æ¬¡æ•°: {trade_count} æ¬¡")
    print(f"ğŸ’µ æœ€ç»ˆå‡€æ”¶ç›Š:   {net_pnl:.2f} bps")
    print(f"ğŸ›‘ ç†”æ–­å™¨æ‹¦æˆªæ¬¡æ•°: {int(df_final['is_phase_collapse'].sum() / 100)} ä¸ªé£é™©çª—å£")
    print("-" * 60)
    print("ğŸ’ è¯„ä»·ï¼šè¿™æ˜¯é€»è¾‘æœ€ä¸¥å¯Œã€é£é™©è°ƒæ•´åæ”¶ç›Šæœ€é«˜çš„ä¸€ç‰ˆã€‚")
    print("="*60)

    return df_final['god_pnl'].cumsum().to_pandas()

# --- Execution Block for RM4BAjRSc_Ye ---
file_path="/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

# Create rqa_results placeholder (cudf.DataFrame)
# Its length should match the full data matrix
# Load full data to get correct length for rqa_results
df_full_matrix_for_rqa_len = cudf.read_parquet(file_path)
rqa_results = cudf.DataFrame({
    'tick_idx': cp.arange(len(df_full_matrix_for_rqa_len)),
    'is_phase_collapse': cp.zeros(len(df_full_matrix_for_rqa_len), dtype=cp.int8)
})

# Call the main function, passing the file_path directly for internal loading
final_equity = gao_hft_god_mode_final(file_path, rqa_results)

"""## Phase Transition"""

import cudf

file_path = "/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

# âœ… å…ˆè¯»å– Parquet æˆ DataFrame
df_gpu = cudf.read_parquet(file_path)

# âœ… å†è°ƒç”¨ GPU RQA
rr_gate, rr_bin, rr_cross, phase_collapse = rqa_phase_transition_gpu(
    df_gpu,
    col_gate='gate_price',
    col_bin='binance_price',
    embed_dim=3,
    tau=1,
    epsilon_scale=1.5
)

def rqa_phase_transition_gpu(df, col_gate='gate_price', col_bin='binance_price',
                             embed_dim=3, tau=1, epsilon_scale=1.5):
    """
    GPU å·¥ä¸šç‰ˆ RQA ç›¸ç©ºé—´åˆ†æ
    - embed_dim: ç›¸ç©ºé—´ç»´åº¦
    - tau: å»¶è¿Ÿæ­¥é•¿
    - epsilon_scale: é˜ˆå€¼å€æ•° (ç›¸å¯¹äº Rolling std)
    """
    print("âš¡ [RQA] GPU Phase Transition åˆ†æå¯åŠ¨...")

    gate = df[col_gate].to_array()
    binance = df[col_bin].to_array()

    # ----------------------
    # 1ï¸âƒ£ ç›¸ç©ºé—´åµŒå…¥
    # ----------------------
    def embed_series(series, m, tau):
        N = len(series) - (m-1)*tau
        return cp.vstack([series[i:N+i*tau] for i in range(m)]).T

    X_gate = embed_series(gate, embed_dim, tau)
    X_bin  = embed_series(binance, embed_dim, tau)

    # ----------------------
    # 2ï¸âƒ£ Recurrence Matrix (GPU)
    # ----------------------
    # ä½¿ç”¨ L2 è·ç¦»
    def recurrence_rate(X, Y=None, epsilon=None):
        if Y is None:
            Y = X
        dist_mat = cp.linalg.norm(X[:, None, :] - Y[None, :, :], axis=2)
        if epsilon is None:
            epsilon = epsilon_scale * cp.std(dist_mat)
        R = (dist_mat <= epsilon).astype(cp.float32)
        return cp.mean(R)  # Recurrence Rate

    # å•ç‹¬ Gate
    rr_gate = recurrence_rate(X_gate)
    # Binance
    rr_bin = recurrence_rate(X_bin)
    # Cross Market
    rr_cross = recurrence_rate(X_gate, X_bin)

    print("ğŸ”¹ RR Gate:", rr_gate.item())
    print("ğŸ”¹ RR Binance:", rr_bin.item())
    print("ğŸ”¹ RR Cross-Market:", rr_cross.item())

    # ----------------------
    # 3ï¸âƒ£ Phase Transition ä¿¡å·
    # ----------------------
    # å½“ Cross-Market RR é«˜äº 0.95ï¼Œæ ‡è®°ç³»ç»ŸåŒæ­¥
    phase_collapse = (rr_cross > 0.95)
    if phase_collapse:
        print("âš ï¸ ç³»ç»Ÿå‘ç”Ÿç¬æ—¶ç›¸å˜! Gate + Binance é«˜åº¦åŒæ­¥ (Flash Crash Risk)")

    return rr_gate.item(), rr_bin.item(), rr_cross.item(), phase_collapse
# =========================
# ğŸ”¹ è°ƒç”¨ç¤ºä¾‹
# =========================
file_path="/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

gao_rqa_phase = rqa_phase_transition_gpu(file_path)

import cudf
import cupy as cp

def gao_rqa_phase_radar_v11(df, window=500, step=100, embed_dim=3, tau=1):
    """
    å·¥ä¸šçº§æ»‘åŠ¨çª—å£ RQA ç›¸å˜åˆ†æ (å†…å­˜ä¼˜åŒ–ç‰ˆ)
    - window: å±€éƒ¨è§‚æµ‹çª—å£ (å¦‚ 500 ticks = 50ç§’)
    - step: æ­¥è¿›ï¼Œå†³å®šæ‰«æé¢‘ç‡
    """
    print(f"âš¡ [RQA-Radar] æ­£åœ¨æ‰«æ {len(df)} è¡Œæ•°æ®ä¸­çš„æ‹“æ‰‘ç›¸å˜ç‚¹...")

    # è‡ªåŠ¨é€‚é…åˆ—å
    gate_col = [c for c in df.columns if 'gate' in c and 'p' in c and 'drift' not in c][0]
    bin_col = [c for c in df.columns if 'bin' in c and 'p' in c and 'drift' not in c][0]

    gate = cp.asarray(df[gate_col].values)
    binance = cp.asarray(df[bin_col].values)

    results = []

    # æ»‘åŠ¨çª—å£æ‰«æ
    for i in range(0, len(gate) - window, step):
        g_win = gate[i : i + window]
        b_win = binance[i : i + window]

        # 1. å¿«é€Ÿç›¸ç©ºé—´åµŒå…¥
        N_sub = window - (embed_dim - 1) * tau
        X_g = cp.vstack([g_win[j : N_sub + j*tau] for j in range(embed_dim)]).T
        X_b = cp.vstack([b_win[j : N_sub + j*tau] for j in range(embed_dim)]).T

        # 2. ä¼˜åŒ–åçš„è·ç¦»çŸ©é˜µè®¡ç®—: ||a-b||^2 = ||a||^2 + ||b||^2 - 2<a,b>
        g_norm = cp.sum(X_g**2, axis=1)[:, None]
        b_norm = cp.sum(X_b**2, axis=1)[None, :]
        dist_sq = g_norm + b_norm - 2 * cp.dot(X_g, X_b.T)
        dist_sq = cp.maximum(dist_sq, 0) # æ¶ˆé™¤æµ®ç‚¹è¯¯å·®

        # 3. è®¡ç®—é€’å½’ç‡ (Cross-Recurrence Rate)
        # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼ epsilon: å±€éƒ¨è·ç¦»å‡å€¼çš„ 0.5 å€
        epsilon_sq = (0.5 * cp.sqrt(cp.mean(dist_sq)))**2
        rr_cross = cp.mean(dist_sq <= epsilon_sq)

        results.append({
            'tick_idx': i + window,
            'rr_cross': float(rr_cross)
        })

        # æ˜¾å­˜ç¢ç‰‡æ¸…ç†
        if i % (step * 50) == 0:
            cp.get_default_memory_pool().free_all_blocks()

    rqa_df = cudf.DataFrame(results)
    # åˆ¤å®šç›¸å˜ï¼šRR çªå¢ä»£è¡¨ç³»ç»Ÿå¤±å»è‡ªç”±åº¦ï¼Œè¿›å…¥â€œåŒæ­¥åç¼©â€
    rqa_df['phase_collapse'] = (rqa_df['rr_cross'] > 0.85).astype(cp.int8)

    print(f"âœ… å®¡è®¡å®Œæˆã€‚è¯†åˆ«åˆ° {rqa_df['phase_collapse'].sum()} å¤„ç¬æ—¶ç›¸å˜ç‚¹ã€‚")
    return rqa_df

# =========================
# ğŸ”¹ è°ƒç”¨ç¤ºä¾‹
# =========================
file_path="/content/drive/MyDrive/6.BTC_Venues/data/processed/BTC_Matrix_100ms_Fixed.parquet"

# Load the DataFrame first
df_rqa = cudf.read_parquet(file_path)
gao_rqa_phase_radar = gao_rqa_phase_radar_v11(df_rqa)

"metadata": {
    "widgets": {
        "some-widget-id": {
            "model_id": "xxx",
            "state": {}   <-- å¦‚æœç¼ºè¿™ä¸ªå°±åŠ ä¸Š
        }
    }
}

import nbformat

with open("Untitled0.ipynb", "r", encoding="utf-8") as f:
    nb = nbformat.read(f, as_version=4)  # v4 æœ€é€šç”¨

# ä¿®å¤ widgets
for cell in nb.cells:
    md = cell.get("metadata", {})
    widgets = md.get("widgets", None)
    if widgets is not None:
        for k, v in widgets.items():
            if "state" not in v:
                v["state"] = {}  # æ·»åŠ ç©º state

# ä¿å­˜ä¿®å¤åçš„ notebook
with open("your_notebook_fixed.ipynb", "w", encoding="utf-8") as f:
    nbformat.write(nb, f)